# modules/ai_post_processor.py - AI-Powered Post-Processing
"""
AI TabanlÄ± Ä°leri DÃ¼zey Post-Processing Sistemi
============================================

Bu modÃ¼l %99.9 doÄŸruluk iÃ§in AI teknolojileri kullanÄ±r:
- GPT-based error correction
- Context-aware spell checking  
- Semantic validation
- Grammar enhancement
- Terminology consistency
- Confidence-based refinement

KullanÄ±m:
    processor = AIPostProcessor()
    corrected_text = processor.enhance_transcription(raw_text, context)
"""

import os
import re
import json
import time
from typing import List, Dict, Optional, Tuple, Any, Set
from dataclasses import dataclass
from collections import defaultdict, Counter
import difflib
import numpy as np

try:
    import openai
    _HAS_OPENAI = True
except ImportError:
    _HAS_OPENAI = False

try:
    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
    import torch
    _HAS_TRANSFORMERS = True
except ImportError:
    _HAS_TRANSFORMERS = False

try:
    import spacy
    _HAS_SPACY = True
except ImportError:
    _HAS_SPACY = False

try:
    from sentence_transformers import SentenceTransformer
    _HAS_SENTENCE_TRANSFORMERS = True
except ImportError:
    _HAS_SENTENCE_TRANSFORMERS = False

# Local imports
try:
    from . import nlp
except ImportError:
    import nlp


@dataclass
class CorrectionCandidate:
    """DÃ¼zeltme adayÄ±"""
    original: str
    corrected: str
    confidence: float
    correction_type: str
    explanation: str
    position: Tuple[int, int]  # (start, end) karakter pozisyonlarÄ±


@dataclass
class SemanticValidationResult:
    """Semantik doÄŸrulama sonucu"""
    is_coherent: bool
    coherence_score: float
    issues: List[str]
    suggestions: List[str]
    topic_consistency: float


@dataclass
class ProcessingResult:
    """Post-processing sonucu"""
    original_text: str
    corrected_text: str
    corrections_made: List[CorrectionCandidate]
    semantic_validation: SemanticValidationResult
    confidence_improvement: float
    processing_stats: Dict[str, Any]


class ContextualSpellChecker:
    """Context-aware yazÄ±m denetleyicisi"""
    
    def __init__(self):
        self.turkish_corrections = self._load_turkish_corrections()
        self.technical_terms = self._load_technical_terms()
        self.common_mistakes = self._load_common_mistakes()
        
    def _load_turkish_corrections(self) -> Dict[str, str]:
        """TÃ¼rkÃ§e yazÄ±m hatalarÄ± ve dÃ¼zeltmeleri"""
        return {
            # YaygÄ±n TÃ¼rkÃ§e yazÄ±m hatalarÄ±
            "degil": "deÄŸil",
            "gordum": "gÃ¶rdÃ¼m", 
            "olabilir": "olabilir",
            "gelicek": "gelecek",
            "gidecegiz": "gideceÄŸiz",
            "yapacagiz": "yapacaÄŸÄ±z",
            "soyle": "ÅŸÃ¶yle",
            "boyle": "bÃ¶yle",
            "sukur": "ÅŸÃ¼kÃ¼r",
            "guzel": "gÃ¼zel",
            "uzgun": "Ã¼zgÃ¼n",
            "uzere": "Ã¼zere",
            "ozellikle": "Ã¶zellikle",
            "cogu": "Ã§oÄŸu",
            "cogul": "Ã§oÄŸul",
            "dogru": "doÄŸru",
            "yanlis": "yanlÄ±ÅŸ",
            "yuzde": "yÃ¼zde",
            "yuzden": "yÃ¼zden",
            
            # Teknik terimler
            "programlama": "programlama",
            "gelistirme": "geliÅŸtirme",
            "uygulamasÄ±": "uygulamasÄ±",
            "cozum": "Ã§Ã¶zÃ¼m",
            "cozmek": "Ã§Ã¶zmek",
            "olcum": "Ã¶lÃ§Ã¼m",
            "olcer": "Ã¶lÃ§er",
            "urun": "Ã¼rÃ¼n",
            "uretim": "Ã¼retim",
            "surec": "sÃ¼reÃ§",
            "surecler": "sÃ¼reÃ§ler",
            
            # Ä°ÅŸ terimleri
            "toplanti": "toplantÄ±",
            "karar": "karar", 
            "kararlar": "kararlar",
            "gorusmek": "gÃ¶rÃ¼ÅŸmek",
            "gorussme": "gÃ¶rÃ¼ÅŸme",
            "degerlendirme": "deÄŸerlendirme",
            "oncelik": "Ã¶ncelik",
            "oncelikli": "Ã¶ncelikli"
        }
        
    def _load_technical_terms(self) -> Set[str]:
        """Teknik terimler sÃ¶zlÃ¼ÄŸÃ¼"""
        return {
            # Teknoloji
            "API", "REST", "JSON", "XML", "HTTP", "HTTPS", 
            "JavaScript", "Python", "Java", "C++", "SQL",
            "database", "framework", "library", "repository",
            "commit", "branch", "merge", "deployment",
            
            # Ä°ÅŸ terimleri
            "meeting", "deadline", "milestone", "deliverable",
            "stakeholder", "feedback", "review", "approval",
            "budget", "timeline", "resource", "scope",
            
            # TÃ¼rkÃ§e teknik
            "yazÄ±lÄ±m", "donanÄ±m", "veritabanÄ±", "sunucu",
            "istemci", "arayÃ¼z", "algoritma", "fonksiyon",
            "deÄŸiÅŸken", "parametre", "dÃ¶ngÃ¼", "koÅŸul"
        }
    
    def _load_common_mistakes(self) -> Dict[str, List[str]]:
        """YaygÄ±n hata tÃ¼rleri ve dÃ¼zeltmeleri"""
        return {
            # Ses benzerliÄŸi hatalarÄ±
            "phonetic": {
                "de": "da",
                "da": "de", 
                "te": "ta",
                "ta": "te",
                "ki": "ke",
                "ke": "ki"
            },
            
            # YazÄ±m hatalarÄ±
            "spelling": {
                "Ã¼": "u",
                "u": "Ã¼",
                "Ã¶": "o", 
                "o": "Ã¶",
                "Ã§": "c",
                "c": "Ã§",
                "ÅŸ": "s",
                "s": "ÅŸ",
                "ÄŸ": "g",
                "g": "ÄŸ",
                "Ä±": "i",
                "i": "Ä±"
            }
        }
    
    def check_and_correct(self, text: str, context: Optional[str] = None) -> List[CorrectionCandidate]:
        """Context-aware yazÄ±m denetimi ve dÃ¼zeltme"""
        corrections = []
        words = text.split()
        
        for i, word in enumerate(words):
            # Temizle (noktalama vs.)
            clean_word = re.sub(r'[^\w]', '', word.lower())
            
            if not clean_word:
                continue
                
            # 1. Direct lookup
            if clean_word in self.turkish_corrections:
                correction = self.turkish_corrections[clean_word]
                if correction != clean_word:
                    corrections.append(CorrectionCandidate(
                        original=word,
                        corrected=self._preserve_case(word, correction),
                        confidence=0.9,
                        correction_type="spelling",
                        explanation=f"YazÄ±m dÃ¼zeltmesi: {clean_word} -> {correction}",
                        position=self._get_word_position(text, i)
                    ))
            
            # 2. Fuzzy matching for similar words
            elif not clean_word in self.technical_terms:
                best_match = self._find_best_fuzzy_match(clean_word)
                if best_match and best_match['confidence'] > 0.8:
                    corrections.append(CorrectionCandidate(
                        original=word,
                        corrected=self._preserve_case(word, best_match['word']),
                        confidence=best_match['confidence'],
                        correction_type="fuzzy_match",
                        explanation=f"YakÄ±n eÅŸleÅŸme: {clean_word} -> {best_match['word']}",
                        position=self._get_word_position(text, i)
                    ))
            
            # 3. Context-based corrections
            if context and i > 0 and i < len(words) - 1:
                context_correction = self._check_context_correction(
                    words[i-1:i+2], i, context
                )
                if context_correction:
                    corrections.append(context_correction)
        
        return corrections
    
    def _preserve_case(self, original: str, corrected: str) -> str:
        """Orijinal kelimenin bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf dÃ¼zenini koru"""
        if original.isupper():
            return corrected.upper()
        elif original.istitle():
            return corrected.capitalize()
        else:
            return corrected
    
    def _find_best_fuzzy_match(self, word: str) -> Optional[Dict]:
        """En iyi yakÄ±n eÅŸleÅŸmeyi bul"""
        best_match = None
        best_ratio = 0.0
        
        # Turkish corrections iÃ§inde ara
        for correct_word in self.turkish_corrections.values():
            ratio = difflib.SequenceMatcher(None, word, correct_word).ratio()
            if ratio > best_ratio and ratio > 0.8:  # Minimum benzerlik
                best_ratio = ratio
                best_match = {
                    'word': correct_word,
                    'confidence': ratio
                }
        
        return best_match
    
    def _get_word_position(self, text: str, word_index: int) -> Tuple[int, int]:
        """Kelimenin metin iÃ§indeki pozisyonunu bul"""
        words = text.split()
        start = 0
        
        for i in range(word_index):
            start = text.find(words[i], start) + len(words[i])
        
        word_start = text.find(words[word_index], start)
        word_end = word_start + len(words[word_index])
        
        return (word_start, word_end)
    
    def _check_context_correction(self, word_triplet: List[str], center_index: int, context: str) -> Optional[CorrectionCandidate]:
        """Context tabanlÄ± dÃ¼zeltme kontrolÃ¼"""
        # Basit context kontrolÃ¼ - geliÅŸtirilebilir
        center_word = word_triplet[1].lower()
        
        # "de/da" durumu
        if center_word in ["de", "da"]:
            prev_word = word_triplet[0].lower() if len(word_triplet) > 0 else ""
            
            # Sesli harf uyumu kontrolÃ¼
            if prev_word:
                last_vowel = self._get_last_vowel(prev_word)
                if last_vowel:
                    correct_suffix = "da" if last_vowel in "aÄ±ou" else "de"
                    if center_word != correct_suffix:
                        return CorrectionCandidate(
                            original=word_triplet[1],
                            corrected=correct_suffix,
                            confidence=0.85,
                            correction_type="context_grammar",
                            explanation=f"Sesli harf uyumu: {center_word} -> {correct_suffix}",
                            position=(0, 0)  # Simplified
                        )
        
        return None
    
    def _get_last_vowel(self, word: str) -> Optional[str]:
        """Kelimenin son sesli harfini bul"""
        vowels = "aeiouÄ±Ã¼Ã¶Ã¼AEIOUÄžÃœÃ–Ä°"
        for char in reversed(word):
            if char in vowels:
                return char.lower()
        return None


class GPTErrorCorrector:
    """GPT tabanlÄ± hata dÃ¼zeltici"""
    
    def __init__(self):
        self.client = None
        if _HAS_OPENAI:
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                openai.api_key = api_key
                self.client = openai
    
    def correct_with_gpt(self, text: str, context: Optional[str] = None, language: str = "tr") -> Optional[CorrectionCandidate]:
        """GPT ile metin dÃ¼zeltme"""
        if not self.client:
            return None
        
        try:
            # Context-aware prompt oluÅŸtur
            prompt = self._create_correction_prompt(text, context, language)
            
            response = self.client.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "Sen bir uzman TÃ¼rkÃ§e dil editÃ¶rÃ¼sÃ¼n. Ses transkripsiyon metinlerindeki hatalarÄ± dÃ¼zeltiyorsun."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.3
            )
            
            corrected_text = response.choices[0].message.content.strip()
            
            # Basit similarity check
            similarity = difflib.SequenceMatcher(None, text, corrected_text).ratio()
            
            if similarity < 0.95:  # EÄŸer deÄŸiÅŸiklik varsa
                return CorrectionCandidate(
                    original=text,
                    corrected=corrected_text,
                    confidence=0.8,
                    correction_type="gpt_correction",
                    explanation="GPT tabanlÄ± genel dÃ¼zeltme",
                    position=(0, len(text))
                )
            
        except Exception as e:
            print(f"GPT dÃ¼zeltme hatasÄ±: {e}")
        
        return None
    
    def _create_correction_prompt(self, text: str, context: Optional[str], language: str) -> str:
        """GPT iÃ§in dÃ¼zeltme prompt'u oluÅŸtur"""
        
        base_prompt = f"""
AÅŸaÄŸÄ±daki ses transkripsiyon metnini dÃ¼zelt. Sadece aÃ§Ä±k hatalarÄ± dÃ¼zelt, anlam deÄŸiÅŸtirme.

DÃ¼zeltilecek metin:
"{text}"
"""
        
        if context:
            base_prompt += f"""
BaÄŸlam bilgisi:
"{context}"
"""
        
        base_prompt += """
Kurallar:
1. Sadece yazÄ±m hatalarÄ±nÄ±, eksik noktalama iÅŸaretlerini ve aÃ§Ä±k gramatikal hatalarÄ± dÃ¼zelt
2. Anlam deÄŸiÅŸtirme, ek kelime ekleme
3. Orijinal konuÅŸma tarzÄ±nÄ± koru  
4. Sadece dÃ¼zeltilmiÅŸ metni dÃ¶ndÃ¼r, aÃ§Ä±klama ekleme

DÃ¼zeltilmiÅŸ metin:
"""
        
        return base_prompt


class SemanticValidator:
    """Semantik tutarlÄ±lÄ±k doÄŸrulayÄ±cÄ±"""
    
    def __init__(self):
        self.sentence_model = None
        if _HAS_SENTENCE_TRANSFORMERS:
            try:
                self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            except Exception:
                self.sentence_model = None
    
    def validate_coherence(self, text: str, context_type: str = "meeting") -> SemanticValidationResult:
        """Metinin semantik tutarlÄ±lÄ±ÄŸÄ±nÄ± doÄŸrula"""
        
        sentences = self._split_into_sentences(text)
        
        if len(sentences) < 2:
            return SemanticValidationResult(
                is_coherent=True,
                coherence_score=1.0,
                issues=[],
                suggestions=[],
                topic_consistency=1.0
            )
        
        issues = []
        suggestions = []
        
        # 1. Sentence coherence check
        coherence_scores = []
        
        if self.sentence_model:
            embeddings = self.sentence_model.encode(sentences)
            
            for i in range(len(sentences) - 1):
                similarity = np.dot(embeddings[i], embeddings[i+1])
                coherence_scores.append(similarity)
                
                if similarity < 0.3:  # DÃ¼ÅŸÃ¼k coherence
                    issues.append(f"CÃ¼mle {i+1} ve {i+2} arasÄ±nda anlam kopukluÄŸu")
                    suggestions.append(f"CÃ¼mleleri gÃ¶zden geÃ§irin: '{sentences[i][:50]}...' ve '{sentences[i+1][:50]}...'")
        
        # 2. Topic consistency
        topic_consistency = self._check_topic_consistency(sentences, context_type)
        
        # 3. Grammar pattern check
        grammar_issues = self._check_basic_grammar(text)
        issues.extend(grammar_issues)
        
        # Overall coherence score
        avg_coherence = np.mean(coherence_scores) if coherence_scores else 0.8
        
        # Final assessment
        is_coherent = (
            avg_coherence > 0.4 and
            topic_consistency > 0.6 and
            len(issues) < len(sentences) * 0.3
        )
        
        return SemanticValidationResult(
            is_coherent=is_coherent,
            coherence_score=avg_coherence,
            issues=issues,
            suggestions=suggestions,
            topic_consistency=topic_consistency
        )
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """Metni cÃ¼mlelere bÃ¶l"""
        # Basit sentence splitting
        sentences = re.split(r'[.!?]+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _check_topic_consistency(self, sentences: List[str], context_type: str) -> float:
        """Konu tutarlÄ±lÄ±ÄŸÄ±nÄ± kontrol et"""
        
        # Context type'a gÃ¶re beklenen kelimeler
        expected_keywords = {
            "meeting": ["toplantÄ±", "karar", "gÃ¶rev", "proje", "takÄ±m", "plan", "tarih"],
            "lecture": ["ders", "konu", "Ã¶ÄŸrenci", "sÄ±nav", "Ã¶dev", "aÃ§Ä±klama", "Ã¶rnek"],
            "interview": ["soru", "cevap", "deneyim", "beceri", "iÅŸ", "ÅŸirket", "pozisyon"]
        }
        
        keywords = expected_keywords.get(context_type, [])
        
        if not keywords:
            return 0.8  # VarsayÄ±lan
        
        # Keyword coverage hesapla
        text_lower = ' '.join(sentences).lower()
        matches = sum(1 for keyword in keywords if keyword in text_lower)
        
        return min(matches / len(keywords), 1.0)
    
    def _check_basic_grammar(self, text: str) -> List[str]:
        """Temel gramer kontrolÃ¼"""
        issues = []
        
        # 1. Tekrarlayan kelimeler
        words = text.split()
        for i in range(len(words) - 1):
            if words[i].lower() == words[i+1].lower():
                issues.append(f"Tekrarlayan kelime: '{words[i]}'")
        
        # 2. Ã‡ok uzun cÃ¼mleler (20+ kelime)
        sentences = self._split_into_sentences(text)
        for i, sentence in enumerate(sentences):
            if len(sentence.split()) > 25:
                issues.append(f"Ã‡ok uzun cÃ¼mle ({len(sentence.split())} kelime): CÃ¼mle {i+1}")
        
        # 3. Ã‡ok kÄ±sa cÃ¼mleler (1-2 kelime)
        for i, sentence in enumerate(sentences):
            if len(sentence.split()) < 3 and len(sentence) > 5:
                issues.append(f"Eksik cÃ¼mle olabilir: '{sentence[:30]}...'")
        
        return issues


class ConfidenceBasedRefiner:
    """GÃ¼ven skoruna dayalÄ± metni iyileÅŸtirici"""
    
    def __init__(self):
        self.low_confidence_patterns = [
            r'\b[a-z]{1,2}\b',  # Ã‡ok kÄ±sa kelimeler
            r'\b\w*\d+\w*\b',   # Rakam karÄ±ÅŸÄ±k kelimeler  
            r'\b[A-Z]{4,}\b',   # Ã‡ok uzun bÃ¼yÃ¼k harfli
            r'\s+',             # Ã‡oklu boÅŸluklar
            r'[^\w\s.,!?;:]'    # Garip karakterler
        ]
    
    def refine_by_confidence(self, text: str, word_confidences: Optional[List[float]] = None) -> Tuple[str, List[CorrectionCandidate]]:
        """GÃ¼ven skoruna gÃ¶re metni iyileÅŸtir"""
        
        corrections = []
        refined_text = text
        
        # 1. Pattern-based low confidence detection
        pattern_issues = self._detect_pattern_issues(text)
        corrections.extend(pattern_issues)
        
        # 2. Word-level confidence issues
        if word_confidences:
            confidence_issues = self._detect_confidence_issues(text, word_confidences)
            corrections.extend(confidence_issues)
        
        # 3. Apply corrections
        for correction in sorted(corrections, key=lambda x: x.confidence, reverse=True):
            if correction.confidence > 0.7:
                refined_text = refined_text.replace(correction.original, correction.corrected)
        
        return refined_text, corrections
    
    def _detect_pattern_issues(self, text: str) -> List[CorrectionCandidate]:
        """Pattern tabanlÄ± sorun tespiti"""
        issues = []
        
        # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
        if re.search(r'\s{2,}', text):
            issues.append(CorrectionCandidate(
                original="multiple_spaces",
                corrected="single_space",
                confidence=0.95,
                correction_type="spacing",
                explanation="Ã‡oklu boÅŸluklar dÃ¼zeltildi",
                position=(0, len(text))
            ))
        
        # Garip karakter kombinasyonlarÄ±
        weird_patterns = re.findall(r'[^\w\s.,!?;:]+', text)
        for pattern in weird_patterns:
            if len(pattern) > 1:
                issues.append(CorrectionCandidate(
                    original=pattern,
                    corrected="",  # KaldÄ±r
                    confidence=0.8,
                    correction_type="character_cleanup",
                    explanation=f"Garip karakter kaldÄ±rÄ±ldÄ±: {pattern}",
                    position=(0, 0)
                ))
        
        return issues
    
    def _detect_confidence_issues(self, text: str, confidences: List[float]) -> List[CorrectionCandidate]:
        """DÃ¼ÅŸÃ¼k gÃ¼ven skorlu kelimeleri tespit et"""
        issues = []
        words = text.split()
        
        if len(words) != len(confidences):
            return issues
        
        for i, (word, conf) in enumerate(zip(words, confidences)):
            if conf < 0.6:  # DÃ¼ÅŸÃ¼k gÃ¼ven
                # Possible corrections based on common issues
                if len(word) == 1 and word.islower():
                    # Single letter might be part of larger word
                    issues.append(CorrectionCandidate(
                        original=word,
                        corrected="[DÃœÅžÃœK_GÃœVENLÄ°]",
                        confidence=conf,
                        correction_type="low_confidence",
                        explanation=f"DÃ¼ÅŸÃ¼k gÃ¼venli kelime (gÃ¼ven: {conf:.2f})",
                        position=(0, 0)
                    ))
        
        return issues


class AIPostProcessor:
    """Ana AI post-processing sistemi"""
    
    def __init__(self):
        self.spell_checker = ContextualSpellChecker()
        self.gpt_corrector = GPTErrorCorrector()
        self.semantic_validator = SemanticValidator()
        self.confidence_refiner = ConfidenceBasedRefiner()
        
    def enhance_transcription(
        self,
        text: str,
        context: Optional[str] = None,
        context_type: str = "meeting",
        word_confidences: Optional[List[float]] = None,
        use_gpt: bool = True,
        language: str = "tr"
    ) -> ProcessingResult:
        """Komprehensif transkripsiyon iyileÅŸtirme"""
        
        start_time = time.time()
        original_text = text
        processing_steps = []
        all_corrections = []
        
        # 1. Contextual spell checking
        print("ðŸ“ Contextual spell checking...")
        spell_corrections = self.spell_checker.check_and_correct(text, context)
        all_corrections.extend(spell_corrections)
        
        # Apply high-confidence spelling corrections
        for correction in spell_corrections:
            if correction.confidence > 0.85:
                text = text.replace(correction.original, correction.corrected)
        
        processing_steps.append("contextual_spell_check")
        
        # 2. Confidence-based refinement
        print("ðŸŽ¯ Confidence-based refinement...")
        refined_text, confidence_corrections = self.confidence_refiner.refine_by_confidence(
            text, word_confidences
        )
        text = refined_text
        all_corrections.extend(confidence_corrections)
        processing_steps.append("confidence_refinement")
        
        # 3. GPT-based correction (optional)
        if use_gpt and self.gpt_corrector.client:
            print("ðŸ¤– GPT-based correction...")
            gpt_correction = self.gpt_corrector.correct_with_gpt(text, context, language)
            if gpt_correction:
                text = gpt_correction.corrected
                all_corrections.append(gpt_correction)
            processing_steps.append("gpt_correction")
        
        # 4. Semantic validation
        print("ðŸ§  Semantic validation...")
        semantic_result = self.semantic_validator.validate_coherence(text, context_type)
        processing_steps.append("semantic_validation")
        
        # 5. Final cleanup
        text = self._final_cleanup(text)
        processing_steps.append("final_cleanup")
        
        # Calculate improvement metrics
        processing_time = time.time() - start_time
        
        # Text similarity (improvement measure)
        similarity = difflib.SequenceMatcher(None, original_text, text).ratio()
        improvement = (1 - similarity) * 100  # Percentage change
        
        processing_stats = {
            'processing_time': processing_time,
            'steps_completed': processing_steps,
            'corrections_applied': len([c for c in all_corrections if c.confidence > 0.7]),
            'total_corrections_found': len(all_corrections),
            'text_length_change': len(text) - len(original_text),
            'similarity_to_original': similarity
        }
        
        return ProcessingResult(
            original_text=original_text,
            corrected_text=text,
            corrections_made=all_corrections,
            semantic_validation=semantic_result,
            confidence_improvement=improvement,
            processing_stats=processing_stats
        )
    
    def _final_cleanup(self, text: str) -> str:
        """Final temizlik iÅŸlemleri"""
        # Multiple spaces to single
        text = re.sub(r'\s+', ' ', text)
        
        # Fix punctuation spacing
        text = re.sub(r'\s+([.,!?;:])', r'\1', text)
        text = re.sub(r'([.,!?;:])\s*', r'\1 ', text)
        
        # Trim
        text = text.strip()
        
        # Capitalize sentences
        sentences = re.split(r'([.!?]+)', text)
        for i in range(0, len(sentences), 2):
            if sentences[i].strip():
                sentences[i] = sentences[i].strip().capitalize()
        
        return ''.join(sentences)


# Kolay kullanÄ±m fonksiyonlarÄ±
def enhance_transcription_text(
    text: str,
    context: Optional[str] = None,
    use_gpt: bool = False,
    language: str = "tr"
) -> str:
    """Tek fonksiyon ile transkripsiyon iyileÅŸtirme"""
    
    processor = AIPostProcessor()
    result = processor.enhance_transcription(
        text=text,
        context=context,
        use_gpt=use_gpt,
        language=language
    )
    
    return result.corrected_text


def get_detailed_corrections(
    text: str,
    context: Optional[str] = None,
    use_gpt: bool = False
) -> ProcessingResult:
    """DetaylÄ± dÃ¼zeltme raporu al"""
    
    processor = AIPostProcessor()
    return processor.enhance_transcription(
        text=text,
        context=context,
        use_gpt=use_gpt
    )


if __name__ == "__main__":
    # Test kodu
    print("ðŸ¤– AI-Powered Post-Processing Test")
    print("=" * 50)
    
    # Test text with intentional errors
    test_text = "Bu bir deneme metnidir. Yazilim gelistirme surecinde onemli olan noktalamÃ¡ iÅŸaretleridir."
    
    processor = AIPostProcessor()
    result = processor.enhance_transcription(test_text)
    
    print(f"Orijinal: {result.original_text}")
    print(f"DÃ¼zeltilmiÅŸ: {result.corrected_text}")
    print(f"DÃ¼zeltme sayÄ±sÄ±: {len(result.corrections_made)}")
    print(f"Semantic tutarlÄ±lÄ±k: {result.semantic_validation.is_coherent}")
    
    available_features = []
    if _HAS_OPENAI: available_features.append("GPT Correction")
    if _HAS_TRANSFORMERS: available_features.append("Transformer Models")
    if _HAS_SPACY: available_features.append("SpaCy NLP")
    if _HAS_SENTENCE_TRANSFORMERS: available_features.append("Sentence Transformers")
    
    print(f"\nâœ… Mevcut Ã¶zellikler: {', '.join(available_features)}")
    print("ðŸš€ AI Post-Processing system ready!")