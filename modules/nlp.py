# modules/nlp.py
from __future__ import annotations

"""
Metin normalizasyonu, Ã¶zetleme ve bilgi Ã§Ä±karÄ±mÄ± (gÃ¶rev / karar) yardÄ±mcÄ±larÄ±.

Ã–ne Ã§Ä±kanlar
------------
- normalize_transcript: Kural tabanlÄ± + (opsiyonel) fuzzy dÃ¼zeltme, tekrar kÄ±rpma,
  noktalama/boÅŸluk toparlama, cÃ¼mle bazlÄ± tekilleÅŸtirme.
- summarize_text: Token-bilinÃ§li parÃ§alara bÃ¶lÃ¼p (overlap ile) ara Ã¶zetler Ã¼retir,
  sonra ikinci geÃ§iÅŸte nihai Ã¶zeti Ã§Ä±karÄ±r. Uzun metinlerde "token indices" uyarÄ±sÄ±nÄ± engeller.
- extract_tasks / extract_decisions: Emir kipleri, aksiyon ve karar kelimeleri, isimle Ã§aÄŸrÄ±
  (Ali:, AyÅŸe,) gibi sinyallere bakarak cÃ¼mle cÃ¼mle gÃ¶rev/karar listesi Ã§Ä±karÄ±r.
- summarize_by_windows: Ã‡ok uzun kayÄ±tlar iÃ§in (Ã¶rn. 10 dk) zaman penceresi bazlÄ± bÃ¶lÃ¼m Ã¶zetleri.

BaÄŸÄ±mlÄ±lÄ±klar
-------------
- transformers  (HF pipeline iÃ§in)
- rapidfuzz     (opsiyonel, fuzzy dÃ¼zeltme â€“ yoksa devre dÄ±ÅŸÄ±)
"""

import os
import re
from typing import Dict, List, Tuple, Iterable, Optional

# ==============================
#  HF Summarizer (lazy load)
# ==============================

_SUMM_MODEL_NAME = "sshleifer/distilbart-cnn-12-6"   # hÄ±zlÄ± ve yeterince iyi; dilersen "facebook/bart-large-cnn"
_SUMMARIZER = None          # transformers.pipeline instance
_TOKENIZER = None           # AutoTokenizer (Ã§oÄŸu pipeline.tokenizer olarak tutar)

def _get_summarizer():
    """
    HF summarizer'Ä± ve tokenizer'Ä± lazy yÃ¼kler.
    DÃ¶nen: (pipeline, tokenizer|None)
    """
    global _SUMMARIZER, _TOKENIZER
    if _SUMMARIZER is None:
        from transformers import pipeline
        _SUMMARIZER = pipeline("summarization", model=_SUMM_MODEL_NAME, device=-1)  # CPU kullan
        _TOKENIZER = getattr(_SUMMARIZER, "tokenizer", None)
    return _SUMMARIZER, _TOKENIZER


# ==============================
#  Opsiyonel fuzzy (rapidfuzz)
# ==============================

try:
    from rapidfuzz import fuzz  # type: ignore
    _HAS_FUZZ = True
except Exception:
    _HAS_FUZZ = False

# Fuzzy yakÄ±nsamasÄ± iÃ§in kanonik terimler (Ã¶zel adlar + teknik terimler)
_CANON_TERMS = [
    "Python", "NumPy", "Pandas", "PostgreSQL", "Docker", "Kubernetes",
    "Arda", "AyÅŸe", "Mehmet", "Zeynep",
    "Ã¶zet", "toplantÄ±", "Ã¶nbellek", "gÃ¼ncelleme", "dokÃ¼man", "metrik",
    "senaryo", "gÃ¶rseller", "kullanÄ±m", "oturum", "Ã¶neri", "teslim", "ekran",
]

# ======================================
#  Kural tabanlÄ± dÃ¼zeltmeler (base fix)
#  (kÃ¶k -> doÄŸru); ek/apos korunur
# ======================================

_BASE_FIX: Dict[str, str] = {
    # Ã–zel isim / teknoloji yanlÄ±ÅŸ okunuÅŸlarÄ±
    "payton": "Python", "paiton": "Python", "paton": "Python", "pyton": "Python", "piton": "Python",
    "numpy": "NumPy", "nampay": "NumPy",
    "pandas": "Pandas",
    "postgre": "PostgreSQL", "postgress": "PostgreSQL",
    "docker": "Docker", "kubernetis": "Kubernetes", "kubernettes": "Kubernetes",

    "marda": "Arda",
    "ayse": "AyÅŸe", "yaÅŸe": "AyÅŸe", "ya iÅŸe": "AyÅŸe",
    "znp": "Zeynep", "znhp": "Zeynep", "zenef": "Zeynep", "znep": "Zeynep", "zenep": "Zeynep",

    # ToplantÄ±/ders baÄŸlamÄ±
    "Ã¶zÃ¼r": "Ã¶zet", "Ã¶zetle": "Ã¶zetle",  # "Ã¶zetle" doÄŸru; normalize ederken korunur
    "toplandÄ±ÄŸÄ±": "toplantÄ±", "toplandigi": "toplantÄ±",
    "uygulamasÄ±ndÄ±k": "uygulamasÄ±ndaki", "uygulamasindÄ±k": "uygulamasÄ±ndaki",

    # Performans/teknik
    "Ã¶nbellik": "Ã¶nbellek", "onbellik": "Ã¶nbellek", "Ã¶n bellek": "Ã¶nbellek", "on bellek": "Ã¶nbellek",
    "gÃ¶rsenleri": "gÃ¶rselleri", "gÃ¶rsenler": "gÃ¶rseller",
    "duvan": "duman", "yÃ¼nceleme": "gÃ¼ncelleme", "gÃ¼nceleme": "gÃ¼ncelleme",
    "metrix": "metrik", "Ã¶nleri": "Ã¶neri", "kullanÄ±lÄ±m": "kullanÄ±m", "otorum": "oturum",
    "Ã§Ä±k deme": "yÃ¼kleme", "geÃ§ Ã§Ä±k": "geÃ§ yÃ¼kleme", "Ã§aÄŸrÄ±ÅŸÄ±mlar": "Ã§aÄŸrÄ±lar",

    # YaygÄ±n konuÅŸma/yazÄ±m hatalarÄ±
    "gÃ¶nderilicek": "gÃ¶nderilecek", "harve": "hale", "topluÄŸa": "topluca", "topluÄŸa at": "topluca at",
    "dÃ¼kÃ¼mana": "dokÃ¼mana", "dÃ¶kÃ¼mana": "dokÃ¼mana",
    "gÃ¶rÃ¼ntÃ¼lÃ¼nÃ¼": "gÃ¶rÃ¼ntÃ¼lerini",
    "ayakkadaÅŸ": "arkadaÅŸ", "dinli": "dilini", "haftalÄ±kla": "haftalÄ±k",
}

# KullanÄ±cÄ± ek sÃ¶zlÃ¼ÄŸÃ¼ (corrections.txt iÃ§eriÄŸi: "yanlÄ±ÅŸ => doÄŸru")
def _load_user_pairs(path: str = "corrections.txt") -> Dict[str, str]:
    pairs: Dict[str, str] = {}
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#") or "=>" not in line:
                    continue
                wrong, right = [x.strip() for x in line.split("=>", 1)]
                if wrong and right:
                    pairs[wrong] = right
    return pairs

# Ekleri/apostrofu koru: Pythonâ€™u, Ã¶nbellekten, AyÅŸeâ€™ye...
_SUFFIX_RE = (
    r"(?P<apos>['â€™]?)"
    r"(?P<suffix>(?:lar|ler|da|de|ta|te|dan|den|tan|ten|a|e|Ä±|i|u|Ã¼|ya|ye|yÄ±|yi|yu|yÃ¼|"
    r"Ä±n|in|un|Ã¼n|sÄ±|si|su|sÃ¼)?)"
)

def _compile_rules(base: Dict[str, str]) -> List[Tuple[re.Pattern, callable]]:
    rules: List[Tuple[re.Pattern, callable]] = []
    for wrong, right in base.items():
        pat = re.compile(rf"\b{re.escape(wrong)}{_SUFFIX_RE}\b", re.IGNORECASE)
        def _mk(right=right):
            return lambda m: f"{right}{m.group('apos')}{m.group('suffix') or ''}"
        rules.append((pat, _mk()))
    return rules

# ==============================
#  Normalize yardÄ±mcÄ±larÄ±
# ==============================

def _normalize_space_punct(t: str) -> str:
    # BoÅŸluklarÄ± sadeleÅŸtir, noktalama etrafÄ±nÄ± toparla
    t = re.sub(r"\s+", " ", t).strip()
    t = re.sub(r"\s+([,;:\.\!\?])", r"\1", t)
    t = re.sub(r"([\(])\s+", r"\1", t)
    t = re.sub(r"\s+([\)])", r"\1", t)
    return t

def _collapse_letter_repeats(t: str) -> str:
    # Harf uzatmalarÄ±nÄ± kÄ±rp: "Ã§oook" -> "Ã§ok"
    return re.sub(r"([A-Za-zÃ‡ÄÄ°Ã–ÅÃœÃ§ÄŸÄ±Ã¶ÅŸÃ¼])\1{2,}", r"\1", t)

def _collapse_word_repeats(t: str) -> str:
    # AynÄ± kelimenin ardÄ±ÅŸÄ±k tekrarlarÄ±nÄ± 2 ile sÄ±nÄ±rla
    words = t.split()
    out, prev, cnt = [], None, 0
    for w in words:
        if w.lower() == (prev or "").lower():
            cnt += 1
            if cnt <= 2:
                out.append(w)
        else:
            prev, cnt = w, 1
            out.append(w)
    return " ".join(out)

def _sentence_split(t: str) -> List[str]:
    # Basit ve dayanÄ±klÄ± cÃ¼mle bÃ¶lÃ¼cÃ¼
    parts = re.split(r"(?<=[\.\!\?])\s+|\n+", (t or "").strip())
    return [p.strip() for p in parts if p.strip()]

def _canon(s: str) -> str:
    # TekilleÅŸtirme anahtarÄ± (noktalama ve boÅŸluklardan arÄ±ndÄ±rÄ±lmÄ±ÅŸ, kÃ¼Ã§Ã¼k harf)
    return re.sub(r"[^\wÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ]+", "", s.lower())

def _fuzzy_fix_token(tok: str) -> str:
    if not _HAS_FUZZ or len(tok) < 3:
        return tok
    m = re.match(rf"^(?P<root>[A-Za-zÃ‡ÄÄ°Ã–ÅÃœÃ§ÄŸÄ±Ã¶ÅŸÃ¼]+){_SUFFIX_RE}$", tok)
    if not m:
        root, apos, suf = tok, "", ""
    else:
        root = m.group("root")
        apos = m.group("apos") or ""
        suf = m.group("suffix") or ""
    best, score = root, 0
    for cand in _CANON_TERMS:
        sc = fuzz.ratio(root.lower(), cand.lower())  # type: ignore
        if sc > score:
            best, score = cand, sc
    return f"{best}{apos}{suf}" if score >= 88 else tok

# ==============================
#  ANA NORMALÄ°ZE
# ==============================

def normalize_transcript(text: str) -> str:
    """
    STT Ã§Ä±ktÄ±sÄ±nÄ± kurumsal metne yaklaÅŸtÄ±rÄ±r:
    - kullanÄ±cÄ± sÃ¶zlÃ¼ÄŸÃ¼ + kural tabanlÄ± dÃ¼zeltme (ek/apos korunur)
    - harf/kelime tekrar kÄ±rpma
    - opsiyonel fuzzy dÃ¼zeltme (rapidfuzz varsa)
    - cÃ¼mle bazlÄ± ardÄ±ÅŸÄ±k tekrar tekilleÅŸtirme
    """
    t = _normalize_space_punct(text or "")
    if not t:
        return ""

    # 1) kullanÄ±cÄ± sÃ¶zlÃ¼ÄŸÃ¼ + baz dÃ¼zeltmeler
    rules = _compile_rules({**_BASE_FIX, **_load_user_pairs()})
    for pat, repl in rules:
        t = pat.sub(repl, t)

    # 2) harf/kelime tekrarlarÄ±
    t = _collapse_letter_repeats(t)
    t = _collapse_word_repeats(t)

    # 3) opsiyonel fuzzy: tek kelimelik tuhaflÄ±klar
    if _HAS_FUZZ:
        t = " ".join(_fuzzy_fix_token(tok) for tok in t.split())

    # 4) cÃ¼mle bazÄ±nda ardÄ±ÅŸÄ±k tekrar kÄ±rpma
    sents = _sentence_split(t)
    out_s, last_key = [], ""
    for s in sents:
        key = _canon(s)
        if not key:
            continue
        if key == last_key:
            continue
        out_s.append(s)
        last_key = key

    return " ".join(out_s)


def normalize_transcript_advanced(text: str, language: str = "tr", fix_spelling: bool = True, fix_foreign_terms: bool = True) -> str:
    """
    GeliÅŸmiÅŸ normalizasyon - Ã§oklu dil desteÄŸi ve yazÄ±m dÃ¼zeltme ile
    
    Args:
        text: Ham transkripsiyon metni
        language: Kaynak dili (tr, en, de, fr, es, it, la)
        fix_spelling: YazÄ±m hatalarÄ±nÄ± dÃ¼zelt
        fix_foreign_terms: YabancÄ± terim dÃ¼zeltme
    """
    if not text or not text.strip():
        return ""
        
    # Temel normalizasyon
    normalized = normalize_transcript(text)
    
    if not fix_spelling and not fix_foreign_terms:
        return normalized
        
    # Dile Ã¶zel dÃ¼zeltmeler
    if language == "tr":
        normalized = _fix_turkish_spelling(normalized, fix_foreign_terms)
    elif language == "en":
        normalized = _fix_english_spelling(normalized)
    elif language in ["de", "fr", "es", "it", "la"]:
        normalized = _fix_foreign_language_spelling(normalized, language)
        
    return normalized


def _load_custom_terms() -> Dict[str, str]:
    """Custom terms dosyasÄ±ndan dÃ¼zeltme Ã§iftleri yÃ¼kle"""
    corrections = {}
    
    # corrections.txt dosyasÄ±ndan
    corrections.update(_load_user_pairs("corrections.txt"))
    
    # custom_terms.txt dosyasÄ±ndan da kelime Ã§iftleri Ã§Ä±kar
    terms_path = "custom_terms.txt"
    if os.path.exists(terms_path):
        with open(terms_path, "r", encoding="utf-8") as f:
            current_section = ""
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    if "# " in line:
                        current_section = line.replace("#", "").strip().lower()
                    continue
                
                # Terim bazlÄ± otomatik dÃ¼zeltme kurallarÄ± oluÅŸtur
                term = line.strip()
                if term and len(term) > 2:
                    # YaygÄ±n yanlÄ±ÅŸ telaffuz/yazÄ±m varyantlarÄ± oluÅŸtur
                    if "Ä°ngilizce" in current_section or "English" in current_section:
                        corrections.update(_generate_english_variants(term))
                    elif "Latince" in current_section or "Latin" in current_section:
                        corrections.update(_generate_latin_variants(term))
                    elif "Almanca" in current_section or "German" in current_section:
                        corrections.update(_generate_german_variants(term))
                    elif "FransÄ±zca" in current_section or "French" in current_section:
                        corrections.update(_generate_french_variants(term))
    
    return corrections


def _generate_english_variants(term: str) -> Dict[str, str]:
    """Ä°ngilizce terim iÃ§in yaygÄ±n yanlÄ±ÅŸ varyantlar oluÅŸtur"""
    variants = {}
    term_lower = term.lower()
    
    # Genel Ä°ngilizce ses deÄŸiÅŸimleri
    common_mistakes = [
        # th -> t/d deÄŸiÅŸimi
        ("th", "t"), ("th", "d"),
        # v -> w deÄŸiÅŸimi  
        ("v", "w"), ("w", "v"),
        # -tion -> -ÅŸÄ±n deÄŸiÅŸimi
        ("tion", "ÅŸÄ±n"), ("tion", "ÅŸan"),
        # double letters
        ("ll", "l"), ("ss", "s"), ("tt", "t"),
        # i/e karÄ±ÅŸÄ±mÄ±
        ("i", "e"), ("e", "i")
    ]
    
    for old, new in common_mistakes:
        if old in term_lower:
            variant = term_lower.replace(old, new)
            if variant != term_lower and len(variant) > 2:
                variants[variant] = term
    
    # Specific corrections
    specific_corrections = {
        "artifishal": "artificial",
        "intelijent": "intelligent", 
        "algoritm": "algorithm",
        "daytabase": "database",
        "servÄ±r": "server",
        "klient": "client",
        "interfeys": "interface",
        "javascirpt": "JavaScript",
        "reakt": "React"
    }
    
    # Terimi specific corrections'ta ara
    for wrong, right in specific_corrections.items():
        if term_lower == right.lower():
            variants[wrong] = term
    
    return variants


def _generate_latin_variants(term: str) -> Dict[str, str]:
    """Latince terim iÃ§in TÃ¼rkÃ§e telaffuz varyantlarÄ±"""
    variants = {}
    term_lower = term.lower()
    
    latin_adaptations = [
        # c -> k/s deÄŸiÅŸimi
        ("c", "k"), ("c", "s"),
        # ae -> e deÄŸiÅŸimi
        ("ae", "e"), ("ae", "a"),
        # ph -> f deÄŸiÅŸimi  
        ("ph", "f"),
        # x -> ks deÄŸiÅŸimi
        ("x", "ks"),
        # qu -> kw deÄŸiÅŸimi
        ("qu", "kw"), ("qu", "ku")
    ]
    
    for old, new in latin_adaptations:
        if old in term_lower:
            variant = term_lower.replace(old, new)
            if variant != term_lower:
                variants[variant] = term
    
    return variants


def _generate_german_variants(term: str) -> Dict[str, str]:
    """Almanca terim iÃ§in TÃ¼rkÃ§e telaffuz varyantlarÄ±"""
    variants = {}
    term_lower = term.lower()
    
    german_adaptations = [
        # sch -> ÅŸ deÄŸiÅŸimi
        ("sch", "ÅŸ"), ("sch", "sh"),
        # ÃŸ -> ss deÄŸiÅŸimi
        ("ÃŸ", "ss"), ("ÃŸ", "s"),
        # Ã¼ -> u deÄŸiÅŸimi
        ("Ã¼", "u"), ("Ã¤", "a"), ("Ã¶", "o"),
        # w -> v deÄŸiÅŸimi
        ("w", "v"), ("v", "w"),
        # z -> ts deÄŸiÅŸimi
        ("z", "ts"), ("z", "s")
    ]
    
    for old, new in german_adaptations:
        if old in term_lower:
            variant = term_lower.replace(old, new)
            if variant != term_lower:
                variants[variant] = term
    
    return variants


def _generate_french_variants(term: str) -> Dict[str, str]:
    """FransÄ±zca terim iÃ§in TÃ¼rkÃ§e telaffuz varyantlarÄ±"""
    variants = {}
    term_lower = term.lower()
    
    french_adaptations = [
        # Silent letters
        ("ent", "an"), ("ent", "ant"),
        # Nasal sounds  
        ("on", "Ä±n"), ("an", "an"),
        ("in", "in"), ("un", "un"),
        # j -> ÅŸ/zh deÄŸiÅŸimi
        ("j", "ÅŸ"), ("j", "zh"), ("j", "c"),
        # ou -> u deÄŸiÅŸimi
        ("ou", "u"), ("au", "o"),
        # Ã§ -> s deÄŸiÅŸimi
        ("Ã§", "s")
    ]
    
    for old, new in french_adaptations:
        if old in term_lower:
            variant = term_lower.replace(old, new)
            if variant != term_lower:
                variants[variant] = term
    
    return variants


def _fix_turkish_spelling(text: str, fix_foreign: bool = True) -> str:
    """TÃ¼rkÃ§e yazÄ±m dÃ¼zeltmeleri - geliÅŸmiÅŸ versiyon"""
    
    # TÃ¼rkÃ§e Ã¶zel dÃ¼zeltmeler
    tr_fixes = {
        # YaygÄ±n hatalÄ± yazÄ±mlar
        "birÅŸey": "bir ÅŸey", "birÅŸeyi": "bir ÅŸeyi", 
        "herÅŸey": "her ÅŸey", "herÅŸeyi": "her ÅŸeyi",
        "birsÃ¼rÃ¼": "bir sÃ¼rÃ¼", "hiÃ§birÅŸey": "hiÃ§ bir ÅŸey",
        
        # Teknik terimler
        "yazilim": "yazÄ±lÄ±m", "geliÅŸitrme": "geliÅŸtirme",
        "uygulamÅŸ": "uygulama", "programÅŸ": "program",
        
        # YaygÄ±n kelime hatalarÄ±
        "hemde": "hem de", "yinede": "yine de", "kesinlikle": "kesinlikle",
        "teÅŸekkÃ¼rler": "teÅŸekkÃ¼rler", "merhebe": "merhaba",
    }
    
    # YabancÄ± terim dÃ¼zeltmeleri
    if fix_foreign:
        # Custom terms dosyasÄ±ndan yÃ¼kle
        custom_corrections = _load_custom_terms()
        tr_fixes.update(custom_corrections)
    
    # DÃ¼zeltmeleri uygula (bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf duyarsÄ±z)
    for wrong, right in tr_fixes.items():
        # Kelime sÄ±nÄ±rlarÄ±na dikkat et
        pattern = r'\b' + re.escape(wrong) + r'\b'
        text = re.sub(pattern, right, text, flags=re.IGNORECASE)
    
    return text


def _fix_english_spelling(text: str) -> str:
    """Ä°ngilizce yazÄ±m dÃ¼zeltmeleri - kapsamlÄ± versiyon"""
    
    # Temel yazÄ±m hatalarÄ±
    en_fixes = {
        # ie/ei karÄ±ÅŸÄ±mlarÄ±
        "recieve": "receive", "recieved": "received", "reciever": "receiver",
        "beleive": "believe", "beleived": "believed", "beleiver": "believer",
        "acheive": "achieve", "acheived": "achieved", "acheiver": "achiever",
        
        # Separate kelimesi
        "seperate": "separate", "seperately": "separately", "seperation": "separation",
        
        # -ly/-ally sonekleri
        "definately": "definitely", "defiantly": "definitely",
        "probaly": "probably", "realy": "really", "usualy": "usually",
        
        # Double letters
        "occured": "occurred", "occurence": "occurrence", "occuring": "occurring",
        "begining": "beginning", "stoped": "stopped", "planed": "planned",
        
        # -ment/-ement karÄ±ÅŸÄ±mÄ±
        "managment": "management", "developement": "development", 
        "arrangment": "arrangement", "judgement": "judgment",
        
        # Environment
        "enviroment": "environment", "enviornment": "environment",
        "goverment": "government", "govenment": "government",
        
        # Teknik terimler
        "algoritm": "algorithm", "algorythm": "algorithm",
        "artifical": "artificial", "artifishal": "artificial",
        "inteligent": "intelligent", "intelijent": "intelligent",
        
        # Akademik terimler
        "knowlege": "knowledge", "knowladge": "knowledge",
        "reserch": "research", "reasearch": "research",
        "analize": "analyze", "analise": "analyze",
        "critisism": "criticism", "critisise": "criticise",
        
        # DiÄŸer yaygÄ±n hatalar
        "wierd": "weird", "freind": "friend", "thier": "their",
        "whitch": "which", "witch": "which", "becuase": "because",
        "necesary": "necessary", "neccessary": "necessary",
        "buisness": "business", "bussiness": "business",
        "intresting": "interesting", "interresting": "interesting"
    }
    
    # Custom terms dosyasÄ±ndan Ä°ngilizce dÃ¼zeltmeler
    custom_terms = _load_custom_terms()
    en_fixes.update(custom_terms)
    
    # DÃ¼zeltmeleri uygula
    for wrong, right in en_fixes.items():
        pattern = r'\b' + re.escape(wrong) + r'\b'
        text = re.sub(pattern, right, text, flags=re.IGNORECASE)
        
    return text


def _fix_foreign_language_spelling(text: str, language: str) -> str:
    """DiÄŸer diller iÃ§in kapsamlÄ± yazÄ±m dÃ¼zeltmeleri"""
    
    # Almanca
    if language == "de":
        de_fixes = {
            # das/dass karÄ±ÅŸÄ±klÄ±ÄŸÄ±
            "dass": "dass", "das": "das",
            # seit/seid karÄ±ÅŸÄ±klÄ±ÄŸÄ±  
            "seit": "seit", "seid": "seid",
            # wird/wirt karÄ±ÅŸÄ±klÄ±ÄŸÄ±
            "wirt": "wird", "wird": "wird",
            # YaygÄ±n yazÄ±m hatalarÄ±
            "gesundheit": "Gesundheit", "gesundhait": "Gesundheit",
            "volkswagen": "Volkswagen", "folkvagen": "Volkswagen", 
            "deutschland": "Deutschland", "doyÃ§land": "Deutschland",
            "kindergarten": "Kindergarten", "kindegarten": "Kindergarten",
            # Umlaut dÃ¼zeltmeleri
            "muench": "MÃ¼nchen", "munchen": "MÃ¼nchen",
            "koeln": "KÃ¶ln", "koln": "KÃ¶ln"
        }
        for wrong, right in de_fixes.items():
            pattern = r'\b' + re.escape(wrong) + r'\b'
            text = re.sub(pattern, right, text, flags=re.IGNORECASE)
    
    # FransÄ±zca
    elif language == "fr":
        fr_fixes = {
            "bonjur": "bonjour", "bonzhur": "bonjour",
            "mersi": "merci", "mersy": "merci", 
            "silvuple": "s'il vous plaÃ®t", "silvu ple": "s'il vous plaÃ®t",
            "reson": "raison", "detr": "d'Ãªtre",
            "se la vi": "c'est la vie", "sela vi": "c'est la vie"
        }
        for wrong, right in fr_fixes.items():
            pattern = r'\b' + re.escape(wrong) + r'\b'
            text = re.sub(pattern, right, text, flags=re.IGNORECASE)
    
    # Ä°spanyolca
    elif language == "es":
        es_fixes = {
            "ola": "hola", "grasiyas": "gracias", "grasyas": "gracias",
            "por favur": "por favor", "porfavor": "por favor",
            "de nade": "de nada", "denade": "de nada",
            "buenas nohes": "buenas noches", "nohes": "noches",
            "asta la vista": "hasta la vista", "astala vista": "hasta la vista"
        }
        for wrong, right in es_fixes.items():
            pattern = r'\b' + re.escape(wrong) + r'\b'
            text = re.sub(pattern, right, text, flags=re.IGNORECASE)
    
    # Ä°talyanca
    elif language == "it":
        it_fixes = {
            "chao": "ciao", "Ã§ao": "ciao",
            "gratsie": "grazie", "gratsye": "grazie",
            "per favore": "per favore", "perfavore": "per favore",
            "molto bene": "molto bene", "moltobene": "molto bene",
            "arrivederci": "arrivederci", "arivederci": "arrivederci"
        }
        for wrong, right in it_fixes.items():
            pattern = r'\b' + re.escape(wrong) + r'\b'
            text = re.sub(pattern, right, text, flags=re.IGNORECASE)
    
    # Latince
    elif language == "la":
        la_fixes = {
            "vise versa": "vice versa", "vise": "vice", "versa": "versa",
            "etselera": "et cetera", "etsetera": "et cetera", "etc": "et cetera",
            "eksampÄ±l gratiya": "exempli gratia", "eksampÄ±l": "exempli", "gratiya": "gratia",
            "perse": "per se", "persente": "per se",
            "adsurdum": "ad absurdum", "adbsurdum": "ad absurdum",
            "sinkua non": "sine qua non", "sinekua non": "sine qua non",
            "defakto": "de facto", "de fakto": "de facto",
            "dejure": "de jure", "de cure": "de jure"
        }
        for wrong, right in la_fixes.items():
            pattern = r'\b' + re.escape(wrong) + r'\b'
            text = re.sub(pattern, right, text, flags=re.IGNORECASE)
    
    return text


# ==============================
#  Ã–ZETLEME (token-bilinÃ§li)
# ==============================

def _token_len(text: str) -> int:
    """
    Tokenizer yoksa kaba tahmin: ~1.3 kelime â‰ˆ 1 token.
    """
    _, tok = _get_summarizer()
    if tok is None:
        return int(max(1, len(text.split()) / 1.3))
    return len(tok.encode(text, add_special_tokens=False))

def _chunks_by_tokens(text: str, max_tokens: int = 500, overlap: int = 50) -> Iterable[str]:
    """
    Metni modelin baÄŸlam sÄ±nÄ±rÄ±nÄ± aÅŸmayacak ÅŸekilde token bazlÄ± parÃ§alara bÃ¶ler.
    Overlap, baÄŸlamÄ±n sÃ¼rekliliÄŸini saÄŸlar.
    """
    summarizer, tok = _get_summarizer()
    if tok is None:
        # Tokenizer yoksa karakter bazlÄ± gÃ¼venli parÃ§alama (daha kÃ¼Ã§Ã¼k)
        chunk_size = 1200
        step = chunk_size - 150
        t = re.sub(r"\s+", " ", (text or "")).strip()
        for i in range(0, len(t), max(1, step)):
            yield t[i:i + chunk_size]
        return

    ids = tok.encode(text, add_special_tokens=False)
    if not ids:
        return
    step = max_tokens - max(0, overlap)
    for start in range(0, len(ids), max(1, step)):
        sub_ids = ids[start:start + max_tokens]
        yield tok.decode(sub_ids, skip_special_tokens=True)

def _safe_summarize_chunk(s: str, max_length: int, min_length: int) -> str:
    """
    Tek bir parÃ§ayÄ± Ã¶zetler; hata/taÅŸma durumunda karakter-bazlÄ± kÃ¼Ã§Ã¼ÄŸe dÃ¼ÅŸer.
    """
    summarizer, _ = _get_summarizer()
    s = s.strip()
    if not s:
        return ""
    try:
        return summarizer(s, max_length=max_length, min_length=min_length, do_sample=False)[0]["summary_text"]
    except Exception:
        # AÅŸÄ±rÄ± uzun/parazitli parÃ§a; biraz kesip tekrar dene
        short = s[:1600]
        return summarizer(short, max_length=max_length, min_length=min_length, do_sample=False)[0]["summary_text"]

def summarize_text(text: str, max_length: Optional[int] = None, min_length: Optional[int] = None, language: str = "tr") -> str:
    """
    Uzun metinleri token-bilinÃ§li parÃ§alara bÃ¶ler, parÃ§a Ã¶zetlerini birleÅŸtirir ve
    ikinci geÃ§iÅŸte nihai Ã¶zeti Ã¼retir. KÄ±sa metinde tek atÄ±ÅŸ yapar.

    Not: Ã‡ok kÄ±sa metinde Ã¶zet anlamsÄ±z olacaÄŸÄ±ndan metnin kendisini dÃ¶ndÃ¼rÃ¼r.
    """
    t = (text or "").strip()
    if not t:
        return ""
    if len(t.split()) < 20:
        return t

    in_tokens = _token_len(t)

    # Hedef uzunluÄŸu girdiye gÃ¶re dinamik belirle
    if max_length is None or min_length is None:
        # Ã‡Ä±kÄ±ÅŸ uzunluÄŸu: giriÅŸ tokenlarÄ±nÄ±n ~%55â€“60'Ä±; sÄ±nÄ±rlar 40â€“190
        max_length = max(40, min(190, int(in_tokens * 0.58)))
        min_length = max(20, int(max_length * 0.45))

    # Tokenizer baÄŸlamÄ±na gÃ¶re gÃ¼venli sÄ±nÄ±r belirle (daha konservatif)
    _, tok = _get_summarizer()
    model_ctx = getattr(tok, "model_max_length", 1024) if tok is not None else 1024
    safe_tokens = min(500, max(200, model_ctx - 200))  # daha bÃ¼yÃ¼k emniyet payÄ±

    if in_tokens <= safe_tokens:
        return _safe_summarize_chunk(t, max_length, min_length)

    # 1) ParÃ§a parÃ§a Ã¶zet
    partials: List[str] = []
    for piece in _chunks_by_tokens(t, max_tokens=safe_tokens, overlap=100):
        ps = _safe_summarize_chunk(piece, max_length, min_length)
        if ps:
            partials.append(ps)

    if not partials:
        return ""

    merged = " ".join(partials)

    # 2) BirleÅŸtirilmiÅŸ ara Ã¶zetleri tekrar kÄ±salt (nihai Ã¶zet)
    final_max = max(80, min(220, int(max_length * 0.9)))
    final_min = max(40, int(final_max * 0.5))
    return _safe_summarize_chunk(merged, final_max, final_min)


def summarize_long_content(text: str, max_length: int = 2000, language: str = "tr", content_mode: str = "auto") -> str:
    """
    Uzun kayÄ±tlar (2-3 saat) iÃ§in geliÅŸmiÅŸ Ã¶zetleme
    
    Args:
        text: Tam transkripsiyon metni
        max_length: Maksimum Ã¶zet uzunluÄŸu (kelime)
        language: Ä°Ã§erik dili
        content_mode: meeting, lecture, interview, auto
    """
    if not text or not text.strip():
        return ""
        
    words = text.split()
    word_count = len(words)
    
    # Ã‡ok uzun iÃ§erik iÃ§in hierarchical summarization
    if word_count > 5000:  # 5000+ kelime iÃ§in chunk-based yaklaÅŸÄ±m
        
        # Ä°Ã§erik tipine gÃ¶re chunk boyutunu ayarla
        if content_mode == "lecture":
            chunk_size = 2000  # Dersler iÃ§in bÃ¼yÃ¼k bÃ¶lÃ¼mler
        elif content_mode == "meeting":
            chunk_size = 1500  # ToplantÄ±lar iÃ§in orta bÃ¶lÃ¼mler  
        else:
            chunk_size = 1800  # VarsayÄ±lan
            
        # Metni anlamlÄ± parÃ§alara bÃ¶l
        chunks = _smart_chunk_text(text, chunk_size)
        
        # Her chunk iÃ§in Ã¶zet
        chunk_summaries = []
        for i, chunk in enumerate(chunks):
            print(f"   ğŸ“‹ BÃ¶lÃ¼m {i+1}/{len(chunks)} Ã¶zetleniyor...")
            
            chunk_summary = summarize_text(
                chunk, 
                max_length=int(max_length / len(chunks) * 1.2),  # Her chunk iÃ§in pay
                language=language
            )
            if chunk_summary:
                chunk_summaries.append(chunk_summary)
        
        # Chunk Ã¶zetlerini birleÅŸtir ve final Ã¶zet
        if chunk_summaries:
            combined = " ".join(chunk_summaries)
            final_summary = summarize_text(
                combined, 
                max_length=max_length,
                language=language
            )
            
            # Ä°Ã§erik tipine gÃ¶re Ã¶zet formatÄ±
            if content_mode == "lecture":
                return _format_lecture_summary(final_summary)
            elif content_mode == "meeting":
                return _format_meeting_summary(final_summary)
            else:
                return final_summary
                
        else:
            return "Ã–zet oluÅŸturulamadÄ±."
    
    else:
        # Daha kÄ±sa iÃ§erik iÃ§in normal Ã¶zetleme
        return summarize_text(text, max_length=max_length, language=language)


def _smart_chunk_text(text: str, target_size: int) -> List[str]:
    """Metni anlamlÄ± noktalarda bÃ¶l (paragraf, cÃ¼mle sÄ±nÄ±rlarÄ±)"""
    
    # Ã–nce paragraflara bÃ¶l
    paragraphs = text.split('\n\n')
    if len(paragraphs) == 1:
        # Paragraf yoksa cÃ¼mlelere bÃ¶l
        sentences = _sentence_split(text)
        return _group_sentences(sentences, target_size)
    
    # ParagraflarÄ± grupla
    chunks = []
    current_chunk = []
    current_size = 0
    
    for para in paragraphs:
        para_size = len(para.split())
        
        if current_size + para_size > target_size and current_chunk:
            # Chunk'Ä± tamamla
            chunks.append(' '.join(current_chunk))
            current_chunk = [para]
            current_size = para_size
        else:
            current_chunk.append(para)
            current_size += para_size
    
    # Son chunk'Ä± ekle
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks


def _group_sentences(sentences: List[str], target_size: int) -> List[str]:
    """CÃ¼mleleri hedef boyutta gruplara topla"""
    
    chunks = []
    current_chunk = []
    current_size = 0
    
    for sentence in sentences:
        sentence_size = len(sentence.split())
        
        if current_size + sentence_size > target_size and current_chunk:
            chunks.append(' '.join(current_chunk))
            current_chunk = [sentence]
            current_size = sentence_size
        else:
            current_chunk.append(sentence)
            current_size += sentence_size
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
        
    return chunks


def _format_lecture_summary(summary: str) -> str:
    """Ders Ã¶zetlerini akademik formatta dÃ¼zenle"""
    return f"ğŸ“š DERS Ã–ZETÄ°\n\n{summary}\n\nğŸ¯ Ana konular ve Ã¶ÄŸrenme hedefleri yukarÄ±da Ã¶zetlenmiÅŸtir."


def _format_meeting_summary(summary: str) -> str:
    """ToplantÄ± Ã¶zetlerini iÅŸ formatÄ±nda dÃ¼zenle"""  
    return f"ğŸ¢ TOPLANTI Ã–ZETÄ°\n\n{summary}\n\nğŸ“‹ Kararlar ve aksiyon maddeleri belirlenen konular dahilindedir."


# ==============================
#  EÄÄ°TÄ°M Ä°Ã‡ERÄ°ÄÄ° MODELLERÄ°
# ==============================

def extract_educational_content(text: str, language: str = "tr") -> Dict:
    """
    EÄŸitim iÃ§eriÄŸi Ã§Ä±karma - dersler iÃ§in Ã¶zelleÅŸtirilmiÅŸ
    
    Returns:
        {
            'topics': List[str],          # Ana konular
            'definitions': List[Dict],    # TanÄ±mlar
            'examples': List[Dict],       # Ã–rnekler
            'questions': List[str],       # Sorular
            'key_points': List[str],      # Ã–nemli noktalar
            'formulas': List[str],        # FormÃ¼ller/denklemler
            'references': List[str]       # Referanslar
        }
    """
    
    result = {
        'topics': [],
        'definitions': [],
        'examples': [],
        'questions': [],
        'key_points': [],
        'formulas': [],
        'references': []
    }
    
    sentences = _sentence_split(text)
    
    for sentence in sentences:
        sent_lower = sentence.lower()
        
        # Konu baÅŸlÄ±klarÄ±nÄ± tespit et
        if _is_topic_header(sentence, language):
            result['topics'].append(sentence.strip())
        
        # TanÄ±mlarÄ± tespit et
        definition = _extract_definition(sentence, language)
        if definition:
            result['definitions'].append(definition)
        
        # Ã–rnekleri tespit et
        example = _extract_example(sentence, language)
        if example:
            result['examples'].append(example)
        
        # SorularÄ± tespit et
        if _is_question(sentence, language):
            result['questions'].append(sentence.strip())
        
        # Ã–nemli noktalarÄ± tespit et
        if _is_key_point(sentence, language):
            result['key_points'].append(sentence.strip())
        
        # FormÃ¼l/denklem tespit et
        formula = _extract_formula(sentence)
        if formula:
            result['formulas'].append(formula)
        
        # Referans tespit et
        reference = _extract_reference(sentence, language)
        if reference:
            result['references'].append(reference)
    
    return result


def _is_topic_header(sentence: str, language: str) -> bool:
    """Konu baÅŸlÄ±ÄŸÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
    
    sent_lower = sentence.lower()
    
    if language == "tr":
        topic_indicators = [
            "konu", "baÅŸlÄ±k", "bÃ¶lÃ¼m", "Ã¼nite", "ders", "konumuz",
            "bugÃ¼nkÃ¼ konu", "ÅŸimdi", "geÃ§iyoruz", "anlatacaÄŸÄ±mÄ±z"
        ]
    else:
        topic_indicators = [
            "topic", "chapter", "section", "subject", "lesson",
            "today's topic", "now we", "moving to", "next topic"
        ]
    
    return any(indicator in sent_lower for indicator in topic_indicators)


def _extract_definition(sentence: str, language: str) -> Optional[Dict]:
    """TanÄ±m Ã§Ä±karma"""
    
    sent_lower = sentence.lower()
    
    if language == "tr":
        definition_patterns = [
            r"(.+?)\s+(dir|dÄ±r|dur|dÃ¼r|demektir|anlamÄ±na gelir|olarak tanÄ±mlanÄ±r)",
            r"(.+?)\s+(nedir|ne demek|ne anlama gelir)",
            r"tanÄ±m\s*:\s*(.+)",
            r"(.+?)\s+dediÄŸimiz"
        ]
    else:
        definition_patterns = [
            r"(.+?)\s+is\s+(defined as|an?)\s+(.+)",
            r"definition\s*:\s*(.+)",
            r"(.+?)\s+means\s+(.+)",
            r"we\s+call\s+(.+?)\s+(.+)"
        ]
    
    for pattern in definition_patterns:
        match = re.search(pattern, sent_lower, re.IGNORECASE)
        if match:
            if len(match.groups()) >= 2:
                term = match.group(1).strip()
                definition = match.group(2).strip() if len(match.groups()) == 2 else match.group(3).strip()
                return {'term': term, 'definition': definition, 'sentence': sentence}
    
    return None


def _extract_example(sentence: str, language: str) -> Optional[Dict]:
    """Ã–rnek Ã§Ä±karma"""
    
    sent_lower = sentence.lower()
    
    if language == "tr":
        example_indicators = [
            "Ã¶rnek", "mesela", "diyelim", "varsayalÄ±m", "farz edelim",
            "Ã¶rneÄŸin", "bir Ã¶rnek", "Ã¶rnekler", "misal"
        ]
    else:
        example_indicators = [
            "example", "for instance", "for example", "such as",
            "let's say", "suppose", "consider", "imagine"
        ]
    
    if any(indicator in sent_lower for indicator in example_indicators):
        return {'text': sentence.strip(), 'type': 'example'}
    
    return None


def _is_question(sentence: str, language: str) -> bool:
    """Soru olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
    
    # Soru iÅŸareti kontrolÃ¼
    if '?' in sentence:
        return True
    
    sent_lower = sentence.lower()
    
    if language == "tr":
        question_starters = [
            "ne", "neden", "nasÄ±l", "nerede", "ne zaman", "kim", "hangi",
            "kaÃ§", "ne kadar", "niye", "niÃ§in"
        ]
    else:
        question_starters = [
            "what", "why", "how", "where", "when", "who", "which",
            "how many", "how much", "can", "is", "are", "do", "does"
        ]
    
    words = sent_lower.split()
    if words and words[0] in question_starters:
        return True
    
    return False


def _is_key_point(sentence: str, language: str) -> bool:
    """Ã–nemli nokta olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
    
    sent_lower = sentence.lower()
    
    if language == "tr":
        key_indicators = [
            "Ã¶nemli", "dikkat", "not", "unutmayÄ±n", "hatÄ±rlayÄ±n",
            "vurgulamak istiyorum", "Ã¶zellikle", "kesinlikle", "mutlaka"
        ]
    else:
        key_indicators = [
            "important", "note", "remember", "keep in mind", "crucial",
            "essential", "key point", "significant", "emphasize"
        ]
    
    return any(indicator in sent_lower for indicator in key_indicators)


def _extract_formula(sentence: str) -> Optional[str]:
    """FormÃ¼l/denklem Ã§Ä±karma"""
    
    # Matematiksel sembol kontrolÃ¼
    math_symbols = ['=', '+', '-', '*', '/', '^', 'âˆš', 'âˆ‘', 'âˆ«', 'Ï€', 'âˆ', 'âˆ†', 'âˆ‡']
    
    if any(symbol in sentence for symbol in math_symbols):
        # Basit formÃ¼l tespiti - daha karmaÅŸÄ±k regex'ler eklenebilir
        formula_pattern = r'[A-Za-z0-9\s]*[=+\-*/^âˆšâˆ‘âˆ«Ï€Î±âˆâˆ†âˆ‡][A-Za-z0-9\s\(\)]*'
        match = re.search(formula_pattern, sentence)
        if match:
            return match.group(0).strip()
    
    return None


def _extract_reference(sentence: str, language: str) -> Optional[str]:
    """Referans Ã§Ä±karma"""
    
    sent_lower = sentence.lower()
    
    if language == "tr":
        ref_indicators = [
            "kaynak", "referans", "literatÃ¼r", "kitap", "makale",
            "yazar", "araÅŸtÄ±rma", "Ã§alÄ±ÅŸma", "sayfa"
        ]
    else:
        ref_indicators = [
            "reference", "source", "literature", "book", "article",
            "author", "research", "study", "page", "according to"
        ]
    
    if any(indicator in sent_lower for indicator in ref_indicators):
        return sentence.strip()
    
    # ISBN, DOI pattern kontrolÃ¼
    isbn_pattern = r'ISBN[:\s]*[\d-]{10,17}'
    doi_pattern = r'DOI[:\s]*[\d\.\/a-zA-Z]+'
    
    if re.search(isbn_pattern, sentence, re.IGNORECASE) or re.search(doi_pattern, sentence, re.IGNORECASE):
        return sentence.strip()
    
    return None


def create_student_summary(text: str, educational_content: Dict, language: str = "tr") -> str:
    """Ã–ÄŸrenciler iÃ§in Ã¶zelleÅŸtirilmiÅŸ Ã¶zet oluÅŸtur"""
    
    if language == "tr":
        summary_parts = ["# ğŸ“š DERS NOTLARI\n"]
        
        # Ana konular
        if educational_content['topics']:
            summary_parts.append("## ğŸ¯ Ä°ÅŸlenen Konular")
            for i, topic in enumerate(educational_content['topics'][:5], 1):
                summary_parts.append(f"{i}. {topic}")
            summary_parts.append("")
        
        # Ã–nemli tanÄ±mlar
        if educational_content['definitions']:
            summary_parts.append("## ğŸ“– Ã–nemli TanÄ±mlar")
            for definition in educational_content['definitions'][:3]:
                summary_parts.append(f"**{definition['term']}:** {definition['definition']}")
            summary_parts.append("")
        
        # Ã–rnekler
        if educational_content['examples']:
            summary_parts.append("## ğŸ’¡ Ã–rnekler")
            for i, example in enumerate(educational_content['examples'][:3], 1):
                summary_parts.append(f"{i}. {example['text']}")
            summary_parts.append("")
        
        # Ã–nemli noktalar
        if educational_content['key_points']:
            summary_parts.append("## âš ï¸ Ã–nemli Noktalar")
            for point in educational_content['key_points'][:5]:
                summary_parts.append(f"â€¢ {point}")
            summary_parts.append("")
        
        # Sorular
        if educational_content['questions']:
            summary_parts.append("## â“ Derste Sorulan Sorular")
            for question in educational_content['questions'][:3]:
                summary_parts.append(f"â€¢ {question}")
            summary_parts.append("")
        
        # FormÃ¼ller
        if educational_content['formulas']:
            summary_parts.append("## ğŸ§® FormÃ¼ller")
            for formula in educational_content['formulas']:
                summary_parts.append(f"â€¢ `{formula}`")
            summary_parts.append("")
        
        # Genel Ã¶zet
        general_summary = summarize_text(text, language=language)
        summary_parts.append("## ğŸ“‹ Genel Ã–zet")
        summary_parts.append(general_summary)
        
    else:  # Ä°ngilizce
        summary_parts = ["# ğŸ“š LECTURE NOTES\n"]
        
        if educational_content['topics']:
            summary_parts.append("## ğŸ¯ Covered Topics")
            for i, topic in enumerate(educational_content['topics'][:5], 1):
                summary_parts.append(f"{i}. {topic}")
            summary_parts.append("")
        
        if educational_content['definitions']:
            summary_parts.append("## ğŸ“– Key Definitions")
            for definition in educational_content['definitions'][:3]:
                summary_parts.append(f"**{definition['term']}:** {definition['definition']}")
            summary_parts.append("")
        
        # DiÄŸer bÃ¶lÃ¼mler benzer ÅŸekilde Ä°ngilizce
        general_summary = summarize_text(text, language=language)
        summary_parts.append("## ğŸ“‹ General Summary")
        summary_parts.append(general_summary)
    
    return "\n".join(summary_parts)


# ==============================
#  GÃ–REV / KARAR Ã‡IKARIMI
# ==============================

_ACTION_WORDS = [
    "yap", "et", "hazÄ±rla", "gÃ¶nder", "kontrol et", "oluÅŸtur", "paylaÅŸ", "ara", "planla",
    "takip et", "tamamla", "gÃ¼ncelle", "raporla", "topla", "deÄŸerlendir", "belirle",
    "kur", "Ã§Ã¶z", "ilet", "onayla", "Ã§Ä±kar", "dÃ¼zenle", "Ã¶lÃ§", "sÄ±kÄ±ÅŸtÄ±r", "yaz", "eÅŸleÅŸtir"
]
_DEADLINE_WORDS = [
    "bugÃ¼n", "yarÄ±n", "haftaya", "son tarih", "deadline", "gÃ¼n sonuna",
    "hafta sonuna", "pazartesi", "salÄ±", "Ã§arÅŸamba", "perÅŸembe", "cuma"
]
_EMIR_KIPI = re.compile(r"(alÄ±m|elim|Ä±nÄ±z|iniz)$", re.IGNORECASE)
_NAME_CALL = re.compile(r"^(ali|ayÅŸe|mehmet|zeynep)\b[,:]?", re.IGNORECASE)

def extract_tasks(text: str, language: str = "tr") -> List[str]:
    """
    Emir kipleri, aksiyon kelimeleri, zaman/dedline sinyalleri ve isimle Ã§aÄŸrÄ± paternine
    gÃ¶re cÃ¼mle bazlÄ± gÃ¶rev listesi Ã§Ä±karÄ±r. TekilleÅŸtirir.
    """
    t = normalize_transcript(text)
    tasks: List[str] = []
    seen: set[str] = set()

    # Dile Ã¶zel aksiyon kelimeleri
    action_words = _ACTION_WORDS
    if language == "en":
        action_words.extend([
            "do", "make", "create", "send", "check", "update", "prepare", "call", 
            "plan", "follow up", "complete", "report", "collect", "evaluate"
        ])
    elif language in ["de", "fr", "es", "it"]:
        # DiÄŸer Avrupa dilleri iÃ§in temel kelimeler
        if language == "de":
            action_words.extend(["machen", "erstellen", "senden", "prÃ¼fen"])
        elif language == "fr":
            action_words.extend(["faire", "crÃ©er", "envoyer", "vÃ©rifier"])

    for sent in _sentence_split(t):
        s = sent.strip()
        if not s:
            continue
        ls = s.lower()

        rule1 = any(w in ls for w in action_words)
        rule2 = "gerekiyor" in ls or "lazÄ±m" in ls or bool(_EMIR_KIPI.search(ls))
        rule3 = any(w in ls for w in _DEADLINE_WORDS)
        rule4 = bool(_NAME_CALL.match(ls))  # "Ali," "AyÅŸe:" vb.

        # Ä°ngilizce iÃ§in ek kurallar
        if language == "en":
            rule2 = rule2 or "need to" in ls or "should" in ls or "must" in ls
            rule3 = rule3 or any(w in ls for w in ["today", "tomorrow", "deadline", "by"])

        if rule1 or rule2 or rule3 or rule4:
            key = _canon(ls)
            if key in seen:
                continue
            seen.add(key)
            tasks.append(s)

    return tasks

_DECISION_WORDS = [
    "karar", "mutabÄ±k", "onaylandÄ±", "kabul edildi", "netleÅŸtirildi", "belirlendi",
    "seÃ§ildi", "uygulanacak", "devreye alÄ±nacak", "sÃ¼rÃ¼m tarihi", "planlandÄ±"
]

def extract_decisions(text: str, language: str = "tr") -> List[str]:
    """
    Karar kelimelerinin geÃ§tiÄŸi cÃ¼mleleri yakalayÄ±p tekilleÅŸtirir.
    """
    t = normalize_transcript(text)
    decisions: List[str] = []
    seen: set[str] = set()

    # Dile Ã¶zel karar kelimeleri
    decision_words = _DECISION_WORDS
    if language == "en":
        decision_words.extend([
            "decided", "agreed", "approved", "determined", "resolved", "concluded",
            "settled", "finalized", "confirmed", "established"
        ])
    elif language == "de":
        decision_words.extend([
            "entschieden", "beschlossen", "vereinbart", "bestimmt"
        ])
    elif language == "fr":
        decision_words.extend([
            "dÃ©cidÃ©", "convenu", "approuvÃ©", "dÃ©terminÃ©"
        ])

    for sent in _sentence_split(t):
        s = sent.strip()
        if not s:
            continue
        ls = s.lower()
        if any(w in ls for w in decision_words):
            key = _canon(ls)
            if key in seen:
                continue
            seen.add(key)
            decisions.append(s)

    return decisions


# ==============================
#  PENCERE Ã–ZETLERÄ° (uzun kayÄ±tlar)
# ==============================

def summarize_by_windows(segments: List[Dict], window_sec: int = 600, language: str = "tr") -> List[Dict]:
    """
    Zaman pencerelerine gÃ¶re (Ã¶rn. 10 dk) Ã¶zet Ã¼retir.
    segments: [{"start": float, "end": float, "text": str}, ...]
    return:  [{"start": sec, "end": sec, "summary": str}]
    """
    if not segments:
        return []

    windows: List[Dict] = []
    bucket: List[Dict] = []
    start, end = 0.0, float(window_sec)

    for s in segments:
        # segment pencereyi aÅŸÄ±yorsa flush et
        while s["end"] > end:
            text = " ".join(x["text"] for x in bucket).strip()
            windows.append({
                "start": start,
                "end": end,
                "summary": summarize_text(text, language=language) if text else ""
            })
            start, end, bucket = end, end + window_sec, []
        bucket.append(s)

    # son kuyruk
    if bucket:
        last_end = segments[-1]["end"]
        text = " ".join(x["text"] for x in bucket).strip()
        windows.append({
            "start": start,
            "end": last_end,
            "summary": summarize_text(text, language=language) if text else ""
        })

    return windows
