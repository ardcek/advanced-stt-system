# modules/stt.py - Enhanced Speech-to-Text Module
"""
GeliÅŸmiÅŸ Ses-Metin DÃ¶nÃ¼ÅŸtÃ¼rme ModÃ¼lÃ¼
====================================

Ã–zellikler:
- Ã‡oklu model desteÄŸi (Whisper, Azure, Google)
- GeliÅŸmiÅŸ ses Ã¶n iÅŸleme (gÃ¼rÃ¼ltÃ¼ azaltma, normalize)
- Adaptif parametre optimizasyonu 
- Kalite deÄŸerlendirme ve gÃ¼ven skoru
- Ä°leri dÃ¼zey post-processing
- KonuÅŸmacÄ± diarization desteÄŸi
- Hibrit model yaklaÅŸÄ±mÄ±

KullanÄ±m:
    result = transcribe_advanced("audio.wav", quality="highest")
    print(f"Transkripsiyon: {result['text']}")
    print(f"GÃ¼ven Skoru: {result['confidence']:.2f}")
"""

import os
import re
import time
import json
import logging
import warnings
import numpy as np
from pathlib import Path
from collections import deque
from typing import Callable, Dict, List, Optional, Tuple, Union, Any
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed

# Suppress warnings
warnings.filterwarnings("ignore")
logging.getLogger().setLevel(logging.WARNING)

# ---- Opsiyonel normalizasyon (proje iÃ§i) ----
try:
    from . import nlp  # normalize_transcript
except Exception:
    nlp = None  # type: ignore

# ---- Audio Processing Imports ----
try:
    import librosa
    import noisereduce as nr
    import webrtcvad
    from pydub import AudioSegment
    from pydub.silence import split_on_silence
    _HAS_AUDIO_PROCESSING = True
except ImportError:
    _HAS_AUDIO_PROCESSING = False

# ---- STT Engines Imports ----
try:
    import azure.cognitiveservices.speech as speechsdk
    _HAS_AZURE = True
except ImportError:
    _HAS_AZURE = False

try:
    from google.cloud import speech
    _HAS_GOOGLE = True
except ImportError:
    _HAS_GOOGLE = False

try:
    from rapidfuzz import fuzz
    _HAS_FUZZ = True
except ImportError:
    _HAS_FUZZ = False

# ---- Data Classes ----
@dataclass
class AudioInfo:
    """Ses dosyasÄ± bilgileri"""
    duration: float
    sample_rate: int
    channels: int
    bit_depth: int
    file_size: int
    snr_estimate: Optional[float] = None
    is_speech: bool = True

@dataclass 
class TranscriptionResult:
    """GeliÅŸmiÅŸ transkripsiyon sonucu"""
    text: str
    segments: List[Dict]
    duration: float
    confidence: float
    audio_info: AudioInfo
    model_used: str
    processing_time: float
    word_level: Optional[List[Dict]] = None
    speaker_info: Optional[List[Dict]] = None
    quality_metrics: Optional[Dict] = None

# ---- YardÄ±mcÄ± Fonksiyonlar ----
def _enhance_meeting_segments(segments: List[Dict]) -> List[Dict]:
    """ToplantÄ± segmentlerine speaker bilgisi ekleme"""
    if not segments:
        return segments
        
    # Basit speaker assignment (gerÃ§ek diarization iÃ§in diarize.py kullanÄ±lacak)
    enhanced = []
    for i, segment in enumerate(segments):
        enhanced_segment = segment.copy()
        # Placeholder speaker assignment
        enhanced_segment['speaker'] = f"Speaker_{(i // 3) % 4 + 1}"  # Basit dÃ¶ngÃ¼
        enhanced.append(enhanced_segment)
    
    return enhanced


def _enhance_lecture_segments(segments: List[Dict]) -> List[Dict]:
    """Ders segmentlerine konu bilgisi ekleme"""
    if not segments:
        return segments
    
    # Ders iÃ§eriÄŸi iÃ§in segment enhancements
    enhanced = []
    for segment in segments:
        enhanced_segment = segment.copy()
        
        # Konu baÅŸlÄ±ÄŸÄ± tespiti (basit heuristic)
        text = segment.get('text', '').lower()
        if any(keyword in text for keyword in ['konu', 'baÅŸlÄ±k', 'bÃ¶lÃ¼m', 'chapter']):
            enhanced_segment['content_type'] = 'topic_header'
        elif any(keyword in text for keyword in ['Ã¶rnek', 'example', 'demo']):
            enhanced_segment['content_type'] = 'example'
        elif any(keyword in text for keyword in ['soru', 'question', '?']):
            enhanced_segment['content_type'] = 'question'
        else:
            enhanced_segment['content_type'] = 'content'
            
        enhanced.append(enhanced_segment)
    
    return enhanced


def _assess_audio_quality(audio_info: AudioInfo) -> str:
    """Ses kalitesi deÄŸerlendirmesi"""
    
    # Ã–rnekleme hÄ±zÄ± kontrolÃ¼
    if audio_info.sample_rate < 16000:
        quality = "DÃ¼ÅŸÃ¼k"
    elif audio_info.sample_rate < 44100:
        quality = "Orta" 
    else:
        quality = "YÃ¼ksek"
    
    # SNR kontrolÃ¼ (eÄŸer hesaplandÄ±ysa)
    if audio_info.snr_estimate:
        if audio_info.snr_estimate < 10:
            quality = "DÃ¼ÅŸÃ¼k (GÃ¼rÃ¼ltÃ¼lÃ¼)"
        elif audio_info.snr_estimate < 20:
            if quality == "YÃ¼ksek":
                quality = "Orta"
        # YÃ¼ksek SNR deÄŸerinde quality deÄŸiÅŸmez
    
    # SÃ¼re faktÃ¶rÃ¼
    if audio_info.duration > 7200:  # 2 saat+
        quality += " (Uzun KayÄ±t)"
    
    return quality


# ==============================
#  INITIALIZATION & SETUP
# ==============================

def initialize():
    """STT motoru baÅŸlatma - modelleri yÃ¼kle ve hazÄ±rlÄ±k yap"""
    print("ğŸš€ STT motoru baÅŸlatÄ±lÄ±yor...")
    
    # Whisper modeli lazy loading iÃ§in hazÄ±rlÄ±k
    global _WHISPER_MODEL_CACHE
    _WHISPER_MODEL_CACHE = {}
    
    # Audio processing kÃ¼tÃ¼phanelerini kontrol et
    if not _HAS_AUDIO_PROCESSING:
        print("âš ï¸  GeliÅŸmiÅŸ ses iÅŸleme kÃ¼tÃ¼phaneleri bulunamadÄ± (librosa, noisereduce)")
        print("   Basit iÅŸleme modu kullanÄ±lacak")
    else:
        print("âœ… Ses iÅŸleme kÃ¼tÃ¼phaneleri hazÄ±r")
    
    # Cloud servis kontrolÃ¼
    if _HAS_AZURE and os.getenv("AZURE_SPEECH_KEY"):
        print("âœ… Azure Speech servis anahtarÄ± bulundu")
    
    if _HAS_GOOGLE and os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
        print("âœ… Google Cloud Speech kimlik bilgileri bulundu")
    
    print("âœ… STT motoru hazÄ±r!")


# Model cache
_WHISPER_MODEL_CACHE = {}


def _canon(s: str) -> str:
    """Metni kanonik forma dÃ¶nÃ¼ÅŸtÃ¼r (karÅŸÄ±laÅŸtÄ±rma iÃ§in)"""
    return re.sub(r"[^\wÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ]+", "", (s or "").lower())

def _norm_txt(txt: str) -> str:
    """Metni normalize et"""
    if not txt:
        return ""
    if nlp and hasattr(nlp, "normalize_transcript"):
        try:
            return nlp.normalize_transcript(txt)
        except Exception:
            pass
    return " ".join((txt or "").split())

def _is_near_duplicate(prev_key: str, cur_key: str, thresh: int = 92) -> bool:
    """Ä°ki metnin benzer olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
    if not prev_key or not cur_key:
        return False
    if prev_key == cur_key:
        return True
    if _HAS_FUZZ:
        return fuzz.ratio(prev_key, cur_key) >= thresh
    return (cur_key in prev_key) or (prev_key in cur_key)

def _estimate_snr(audio_path: str) -> Optional[float]:
    """Ses dosyasÄ±nÄ±n sinyal-gÃ¼rÃ¼ltÃ¼ oranÄ±nÄ± tahmin et"""
    if not _HAS_AUDIO_PROCESSING:
        return None
    try:
        y, sr = librosa.load(audio_path, sr=None)
        # Basit SNR tahmini: RMS oranÄ± kullanarak
        rms = librosa.feature.rms(y=y)[0]
        signal_power = np.mean(rms**2)
        noise_power = np.min(rms**2) 
        if noise_power > 0:
            snr_db = 10 * np.log10(signal_power / noise_power)
            return max(0, min(60, snr_db))  # 0-60 dB arasÄ±nda sÄ±nÄ±rla
    except Exception:
        pass
    return None

def _get_audio_info(audio_path: str) -> AudioInfo:
    """Ses dosyasÄ± hakkÄ±nda detaylÄ± bilgi al"""
    try:
        if _HAS_AUDIO_PROCESSING:
            # Pydub ile temel bilgiler
            audio = AudioSegment.from_file(audio_path)
            duration = len(audio) / 1000.0  # ms to seconds
            sample_rate = audio.frame_rate
            channels = audio.channels
            bit_depth = audio.sample_width * 8
            file_size = os.path.getsize(audio_path)
            
            # SNR tahmini
            snr = _estimate_snr(audio_path)
            
            return AudioInfo(
                duration=duration,
                sample_rate=sample_rate, 
                channels=channels,
                bit_depth=bit_depth,
                file_size=file_size,
                snr_estimate=snr,
                is_speech=True  # VAD ile belirlenebilir
            )
        else:
            # Fallback: dosya boyutu ve temel tahmin
            file_size = os.path.getsize(audio_path)
            return AudioInfo(
                duration=0.0,
                sample_rate=16000,
                channels=1,
                bit_depth=16,
                file_size=file_size
            )
    except Exception:
        return AudioInfo(
            duration=0.0,
            sample_rate=16000, 
            channels=1,
            bit_depth=16,
            file_size=0
        )

def _preprocess_audio(input_path: str, output_path: Optional[str] = None) -> str:
    """Ses dosyasÄ±nÄ± transkripsiyon iÃ§in optimize et"""
    if not _HAS_AUDIO_PROCESSING:
        return input_path
        
    if output_path is None:
        output_path = input_path.replace('.wav', '_processed.wav')
    
    try:
        # Ses dosyasÄ±nÄ± yÃ¼kle
        y, sr = librosa.load(input_path, sr=None)
        
        # 1. GÃ¼rÃ¼ltÃ¼ azaltma
        y_denoised = nr.reduce_noise(y=y, sr=sr, stationary=False, prop_decrease=0.8)
        
        # 2. Normalize etme
        y_normalized = librosa.util.normalize(y_denoised)
        
        # 3. Mono'ya Ã§evir (eÄŸer stereo ise)
        if len(y_normalized.shape) > 1:
            y_normalized = librosa.to_mono(y_normalized)
        
        # 4. 16kHz'e resample (Whisper iÃ§in optimum)
        if sr != 16000:
            y_resampled = librosa.resample(y_normalized, orig_sr=sr, target_sr=16000)
            sr = 16000
        else:
            y_resampled = y_normalized
        
        # 5. Sessizlikleri kÄ±rp
        y_trimmed, _ = librosa.effects.trim(y_resampled, top_db=20)
        
        # Ä°ÅŸlenmiÅŸ ses dosyasÄ±nÄ± kaydet
        import soundfile as sf
        sf.write(output_path, y_trimmed, sr)
        
        print(f"[STT] Ses Ã¶n iÅŸleme tamamlandÄ±: {input_path} -> {output_path}")
        return output_path
        
    except Exception as e:
        print(f"[STT] Ses Ã¶n iÅŸleme baÅŸarÄ±sÄ±z: {e}")
        return input_path

def _adaptive_parameters(audio_info: AudioInfo) -> Dict[str, Any]:
    """Ses dosyasÄ± Ã¶zelliklerine gÃ¶re optimum parametreleri belirle"""
    params = {
        'beam_size': 2,
        'temperature': 0.0,
        'compression_ratio_threshold': 2.4,
        'no_speech_threshold': 0.5,
        'vad_min_silence_ms': 300
    }
    
    # SNR'a gÃ¶re ayarlama
    if audio_info.snr_estimate:
        if audio_info.snr_estimate < 10:  # DÃ¼ÅŸÃ¼k kalite
            params['beam_size'] = 5  # Daha dikkatli
            params['temperature'] = [0.0, 0.2]  # Fallback ile
            params['no_speech_threshold'] = 0.4
        elif audio_info.snr_estimate > 30:  # YÃ¼ksek kalite
            params['beam_size'] = 1  # HÄ±zlÄ±
            params['no_speech_threshold'] = 0.6
    
    # SÃ¼reye gÃ¶re ayarlama
    if audio_info.duration > 3600:  # 1 saattan uzun
        params['vad_min_silence_ms'] = 500  # Daha agresif VAD
    elif audio_info.duration < 60:  # 1 dakikadan kÄ±sa
        params['vad_min_silence_ms'] = 100  # Hassas VAD
    
    return params

# ---- GeliÅŸmiÅŸ Terim ve Prompt YÃ¶netimi ----
_DEFAULT_TERMS = [
    "Python", "NumPy", "Pandas", "PostgreSQL", "Docker", "Kubernetes",
    "toplantÄ±", "Ã¶zet", "aksiyon", "karar", "gÃ¶rev",
    "Ã¶nbellek", "bildirim", "oturum", "kullanÄ±cÄ±", "sistem",
    "gÃ¼ncelleme", "dokÃ¼man", "metrik", "performans", "optimizasyon",
    "veritabanÄ±", "API", "framework", "library", "deployment"
]

def _load_custom_terms(path: str = "custom_terms.txt") -> List[str]:
    """KullanÄ±cÄ± Ã¶zel terimlerini yÃ¼kle"""
    terms = list(_DEFAULT_TERMS)
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                t = line.strip()
                if t and not t.startswith("#") and t not in terms:
                    terms.append(t)
    return terms

def _generate_smart_prompt(language: str = "tr", audio_info: Optional[AudioInfo] = None) -> str:
    """Ses Ã¶zelliklerine gÃ¶re akÄ±llÄ± prompt Ã¼ret"""
    terms = ", ".join(_load_custom_terms())
    
    base_prompt = ""
    if language.lower().startswith("tr"):
        base_prompt = (
            "Bu kayÄ±t TÃ¼rkÃ§e bir profesyonel toplantÄ±/eÄŸitimdir. "
            "Noktalama iÅŸaretlerini doÄŸru kullan, Ã¶zel isimleri ve teknik terimleri "
            "tam olarak yaz. Her kelimenin en doÄŸru halini tercih et. "
        )
    else:
        base_prompt = (
            "This is a professional meeting/training recording in Turkish. "
            "Use correct punctuation, write proper nouns and technical terms accurately. "
            "Prefer the most accurate form of each word. "
        )
    
    # Ses kalitesine gÃ¶re ek talimatlar
    if audio_info and audio_info.snr_estimate:
        if audio_info.snr_estimate < 15:
            base_prompt += "Ses kalitesi dÃ¼ÅŸÃ¼k, belirsiz sesleri tahmin etme. "
        elif audio_info.snr_estimate > 35:
            base_prompt += "Ses kalitesi yÃ¼ksek, detaylarÄ± kaÃ§Ä±rma. "
    
    return base_prompt + f"Domain terimleri: {terms}."

def _calculate_confidence(segments: List[Dict], model_used: str) -> float:
    """Transkripsiyon gÃ¼ven skorunu hesapla"""
    if not segments:
        return 0.0
    
    confidences = []
    for seg in segments:
        # Segment bazÄ±nda gÃ¼ven skoru faktÃ¶rleri
        text = seg.get('text', '')
        
        # 1. Metin uzunluÄŸu faktÃ¶rÃ¼
        length_factor = min(1.0, len(text.split()) / 10)
        
        # 2. Ã–zel terim varlÄ±ÄŸÄ±
        terms_found = sum(1 for term in _load_custom_terms() 
                         if term.lower() in text.lower())
        term_factor = min(1.0, terms_found / 3)
        
        # 3. Model bazlÄ± temel gÃ¼ven
        base_confidence = 0.85 if 'whisper' in model_used.lower() else 0.75
        
        # 4. Tekrar / dublicate yokluÄŸu
        no_repeat_factor = 1.0 if len(set(text.split())) == len(text.split()) else 0.8
        
        segment_conf = base_confidence * (0.4 + 0.3 * length_factor + 
                                        0.2 * term_factor + 0.1 * no_repeat_factor)
        confidences.append(segment_conf)
    
    return sum(confidences) / len(confidences) if confidences else 0.0

# =============================================================================
#                           GPU (PyTorch / openai-whisper)
# =============================================================================
def _transcribe_whisper_gpu(
    path: str,
    language: str,
    model_size: str,
    progress: bool,
) -> Dict:
    import torch
    import whisper  # openai-whisper

    if not torch.cuda.is_available():
        raise RuntimeError("CUDA not available for PyTorch")

    print("[STT][GPU] openai-whisper baÅŸlatÄ±lÄ±yorâ€¦")
    t0 = time.time()
    model = whisper.load_model(model_size, device="cuda")

    result = model.transcribe(
        audio=path,
        language=language,
        task="transcribe",
        initial_prompt=_generate_smart_prompt(language),
        fp16=True,                       # RTX 4060 iÃ§in ideal
        temperature=0.0,                 # tek deneme, hÄ±zlÄ±
        condition_on_previous_text=True,
        no_speech_threshold=0.5,
        compression_ratio_threshold=2.4,
        logprob_threshold=-1.0,
        word_timestamps=False,           # hÄ±z iÃ§in kapalÄ±
        verbose=False,
    )

    duration = float(result.get("duration") or 0.0)
    segs_in: List[dict] = result.get("segments", []) or []
    out_segments: List[Dict] = []
    full: List[str] = []

    last_key = ""
    for s in segs_in:
        txt_raw = (s.get("text") or "").strip()
        if not txt_raw:
            continue
        txt = _norm_txt(txt_raw)
        key = _canon(txt)
        if _is_near_duplicate(last_key, key):
            continue
        last_key = key

        st = float(s.get("start") or 0.0)
        en = float(s.get("end") or st)

        if progress and duration > 0:
            pct = min(100, int((en / duration) * 100))
            print(f"[STT] {en:6.1f}s / {duration:6.1f}s  ({pct:3d}%)")

        out_segments.append({"start": st, "end": en, "text": txt, "words": []})
        full.append(txt)

    t1 = time.time()
    if duration > 0:
        rtf = (t1 - t0) / duration
        print(f"[STT][GPU] tamamlandÄ± â€” sÃ¼re: {t1 - t0:.1f}s, kayÄ±t uzunluÄŸu: {duration:.1f}s, RTF: {rtf:.2f}")

    return {"text": " ".join(full).strip(), "segments": out_segments, "duration": duration}

# =============================================================================
#                         CPU (faster-whisper / CTranslate2)
# =============================================================================
_FW_MODEL: Optional[Any] = None  # WhisperModel instance
_FW_CFG: Optional[Tuple[str, str, str]] = None  # (size, device, compute_type)

def _get_fw_model(size: str, device: str, compute: Optional[str]):
    from faster_whisper import WhisperModel
    global _FW_MODEL, _FW_CFG
    if compute is None:
        compute = "int8" if device == "cpu" else "float16"
    cfg = (size, device, compute)
    if _FW_MODEL is None or _FW_CFG != cfg:
        _FW_MODEL = WhisperModel(size, device=device, compute_type=compute)
        _FW_CFG = cfg
    return _FW_MODEL

def _transcribe_fw_cpu(
    path: str,
    language: str,
    model_size: str,
    device: str,
    beam_size: int,
    vad_min_silence_ms: int,
    progress: bool,
) -> Dict:
    model = _get_fw_model(model_size, device=device, compute=None)

    t0 = time.time()
    segments_gen, info = model.transcribe(
        path,
        language=language,
        beam_size=beam_size,                          # 2 hÄ±zlÄ±, 5 daha isabetli
        vad_filter=True,
        vad_parameters={"min_silence_duration_ms": vad_min_silence_ms},
        condition_on_previous_text=True,
        initial_prompt=_generate_smart_prompt(language),
        no_speech_threshold=0.5,
        compression_ratio_threshold=2.4,
        log_prob_threshold=-1.0,
        temperature=[0.0],                            # fallback kapalÄ± â†’ stabil hÄ±z
        word_timestamps=False,                        # hÄ±z
    )

    total = float(getattr(info, "duration", 0.0) or 0.0)
    out_segments: List[Dict] = []
    full_text_parts: List[str] = []
    last_keys = deque(maxlen=6)
    last_end = 0.0

    for seg in segments_gen:
        raw = (seg.text or "").strip()
        if not raw:
            continue

        norm = _norm_txt(raw)
        key = _canon(norm)
        if key and any(_is_near_duplicate(prev, key) for prev in last_keys):
            if progress and total > 0 and seg.end:
                pct = min(100, int((float(seg.end) / total) * 100))
                print(f"[STT] skip {seg.end:6.1f}s / {total:6.1f}s  ({pct:3d}%)  â€” dedupe")
            continue
        last_keys.append(key)

        s_start = float(seg.start) if seg.start is not None else last_end
        s_end   = float(seg.end)   if seg.end   is not None else s_start
        last_end = s_end

        if progress and total > 0:
            pct = min(100, int((s_end / total) * 100))
            print(f"[STT] {s_end:6.1f}s / {total:6.1f}s  ({pct:3d}%)")

        out_segments.append({"start": s_start, "end": s_end, "text": norm, "words": []})
        full_text_parts.append(norm)

    dt = time.time() - t0
    if progress and total > 0:
        rtf = dt / total
        print(f"[STT][CPU] tamamlandÄ± â€” sÃ¼re: {dt:.1f}s, kayÄ±t uzunluÄŸu: {total:.1f}s, RTF: {rtf:.2f}")

    # tek tur daha nazik dedupe
    cleaned_segments: List[Dict] = []
    prev_key = ""
    for sd in out_segments:
        k = _canon(sd["text"])
        if k and _is_near_duplicate(prev_key, k):
            continue
        cleaned_segments.append(sd)
        prev_key = k

    text = " ".join(s["text"] for s in cleaned_segments).strip()
    return {"text": text, "segments": cleaned_segments, "duration": total}

# =============================================================================
#                              ORTAK API
# =============================================================================
def transcribe(
    path: str,
    language: str = "tr",
    model_size: str = "medium",
    device: str = "cpu",
    beam_size: int = 2,
    vad_min_silence_ms: int = 300,
    progress: bool = True,
    on_segment: Optional[Callable[[Dict], None]] = None,  # (CPU yolu kullanÄ±r)
) -> Dict:
    """
    device='cuda' + PyTorch CUDA True â†’ openai-whisper (GPU)
    aksi halde â†’ faster-whisper (CPU)
    """
    if device == "cuda":
        try:
            import torch  # gecikmeli import
            if torch.cuda.is_available():
                return _transcribe_whisper_gpu(path, language, model_size, progress)
        except Exception as e:
            print(f"[STT] GPU yolu baÅŸarÄ±sÄ±z: {e}. CPU yoluna dÃ¼ÅŸÃ¼lÃ¼yorâ€¦")

    # CPU yolu
    return _transcribe_fw_cpu(
        path=path,
        language=language,
        model_size=model_size,
        device="cpu",
        beam_size=beam_size,
        vad_min_silence_ms=vad_min_silence_ms,
        progress=progress,
    )

# =============================================================================
#                           AZURE SPEECH API
# =============================================================================
def _transcribe_azure(
    path: str,
    language: str = "tr-TR", 
    speech_key: Optional[str] = None,
    region: str = "eastus"
) -> Dict:
    """Azure Cognitive Services Speech-to-Text"""
    if not _HAS_AZURE:
        raise ImportError("Azure Speech SDK not available")
    
    # API key kontrolÃ¼
    if not speech_key:
        speech_key = os.getenv("AZURE_SPEECH_KEY")
    if not speech_key:
        raise ValueError("Azure Speech key required")
    
    try:
        t0 = time.time()
        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=region)
        speech_config.speech_recognition_language = language
        
        # GeliÅŸmiÅŸ ayarlar
        speech_config.set_property(speechsdk.PropertyId.Speech_SegmentationSilenceTimeoutMs, "300")
        speech_config.set_property(speechsdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, "300")
        
        audio_config = speechsdk.audio.AudioConfig(filename=path)
        speech_recognizer = speechsdk.SpeechRecognizer(
            speech_config=speech_config, 
            audio_config=audio_config
        )
        
        segments = []
        full_text = []
        current_time = 0.0
        
        def recognized_handler(evt):
            nonlocal current_time
            if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:
                text = evt.result.text.strip()
                if text:
                    # Azure zamanlamasÄ± tam deÄŸil, tahmini hesapla
                    duration = len(text.split()) * 0.5  # ~0.5 saniye/kelime
                    segments.append({
                        "start": current_time,
                        "end": current_time + duration,
                        "text": text,
                        "words": []
                    })
                    full_text.append(text)
                    current_time += duration
        
        speech_recognizer.recognized.connect(recognized_handler)
        
        # SÃ¼rekli tanÄ±ma baÅŸlat
        speech_recognizer.start_continuous_recognition()
        
        # Ses dosyasÄ±nÄ±n sÃ¼resini bekle (basit yaklaÅŸÄ±m)
        import time
        time.sleep(10)  # TODO: GerÃ§ek ses uzunluÄŸuna gÃ¶re ayarla
        
        speech_recognizer.stop_continuous_recognition()
        
        duration = time.time() - t0
        print(f"[STT][Azure] tamamlandÄ± â€” sÃ¼re: {duration:.1f}s")
        
        return {
            "text": " ".join(full_text).strip(),
            "segments": segments,
            "duration": current_time
        }
        
    except Exception as e:
        print(f"[STT][Azure] hata: {e}")
        raise

# =============================================================================
#                           GOOGLE CLOUD SPEECH API
# =============================================================================
def _transcribe_google(
    path: str,
    language: str = "tr-TR",
    credentials_path: Optional[str] = None
) -> Dict:
    """Google Cloud Speech-to-Text"""
    if not _HAS_GOOGLE:
        raise ImportError("Google Cloud Speech not available")
    
    try:
        t0 = time.time()
        
        # Credentials ayarlama
        if credentials_path:
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = credentials_path
        
        client = speech.SpeechClient()
        
        # Ses dosyasÄ±nÄ± oku
        with open(path, "rb") as audio_file:
            content = audio_file.read()
        
        audio = speech.RecognitionAudio(content=content)
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=16000,
            language_code=language,
            enable_automatic_punctuation=True,
            enable_word_time_offsets=True,
            enable_speaker_diarization=False,
            model="latest_long",
            use_enhanced=True
        )
        
        # Uzun ses iÃ§in async iÅŸlem
        if os.path.getsize(path) > 10 * 1024 * 1024:  # 10MB+
            operation = client.long_running_recognize(config=config, audio=audio)
            response = operation.result(timeout=300)  # 5 dakika timeout
        else:
            response = client.recognize(config=config, audio=audio)
        
        segments = []
        full_text = []
        
        for result in response.results:
            alternative = result.alternatives[0]
            text = alternative.transcript.strip()
            
            if text:
                # Word timing bilgilerini kullan
                start_time = 0.0
                end_time = 0.0
                
                if hasattr(alternative, 'words') and alternative.words:
                    start_time = alternative.words[0].start_time.total_seconds()
                    end_time = alternative.words[-1].end_time.total_seconds()
                
                segments.append({
                    "start": start_time,
                    "end": end_time,
                    "text": text,
                    "words": [
                        {
                            "word": w.word,
                            "start": w.start_time.total_seconds(),
                            "end": w.end_time.total_seconds(),
                            "confidence": getattr(w, 'confidence', 0.9)
                        }
                        for w in getattr(alternative, 'words', [])
                    ]
                })
                full_text.append(text)
        
        duration = segments[-1]["end"] if segments else 0.0
        processing_time = time.time() - t0
        
        print(f"[STT][Google] tamamlandÄ± â€” sÃ¼re: {processing_time:.1f}s, kayÄ±t uzunluÄŸu: {duration:.1f}s")
        
        return {
            "text": " ".join(full_text).strip(),
            "segments": segments,
            "duration": duration
        }
        
    except Exception as e:
        print(f"[STT][Google] hata: {e}")
        raise

# =============================================================================
#                        HÄ°BRÄ°T MODEL YAKLAÅIMI
# =============================================================================
def _transcribe_hybrid(
    path: str,
    language: str = "tr",
    models: List[str] = ["whisper", "azure"],
    **kwargs
) -> Dict:
    """Ã‡oklu model kullanarak hibrit transkripsiyon"""
    results = []
    
    for model_name in models:
        try:
            print(f"[STT][Hibrit] {model_name} modeli deneniyor...")
            
            if model_name == "whisper":
                result = transcribe(path, language=language, **kwargs)
            elif model_name == "azure" and _HAS_AZURE:
                result = _transcribe_azure(path, f"{language}-TR" if language == "tr" else language)
            elif model_name == "google" and _HAS_GOOGLE:
                result = _transcribe_google(path, f"{language}-TR" if language == "tr" else language)
            else:
                continue
            
            result["model"] = model_name
            results.append(result)
            
        except Exception as e:
            print(f"[STT][Hibrit] {model_name} baÅŸarÄ±sÄ±z: {e}")
            continue
    
    if not results:
        raise RuntimeError("HiÃ§bir model baÅŸarÄ±lÄ± olamadÄ±")
    
    # En iyi sonucu seÃ§ (uzunluk ve gÃ¼ven bazÄ±nda)
    best_result = max(results, key=lambda r: len(r.get("text", "")))
    
    print(f"[STT][Hibrit] En iyi model: {best_result.get('model', 'unknown')}")
    return best_result

# =============================================================================
#                          GELÄ°ÅMÄ°Å ANA API  
# =============================================================================
def transcribe_advanced(
    path: str,
    language: str = "tr",
    quality: str = "ultra",  # "fastest", "balanced", "highest", "ultra", "hybrid"
    preprocess: bool = True,
    engine: str = "auto",  # "whisper", "azure", "google", "hybrid", "auto"
    model_name: Optional[str] = None,  # Model adÄ± (large-v3, medium, vs.)
    device: str = "cpu",  # cpu/cuda
    content_type: str = "auto",  # meeting, lecture, interview, auto
    long_form: bool = False,  # Uzun kayÄ±t optimizasyonu
    beam_size: Optional[int] = None,  # Beam search boyutu
    vad_threshold: Optional[float] = None,  # VAD eÅŸiÄŸi
    **kwargs
) -> TranscriptionResult:
    """
    GeliÅŸmiÅŸ transkripsiyon API
    
    Args:
        path: Ses dosyasÄ± yolu
        language: Dil kodu (tr, en, vs.)
        quality: Kalite seviyesi
            - fastest: En hÄ±zlÄ± (small model, minimal iÅŸlem)
            - balanced: Dengeli (medium model, Ã¶n iÅŸleme)
            - highest: En yÃ¼ksek (large model, tÃ¼m Ã¶zellikler)
            - hybrid: Ã‡oklu model karÅŸÄ±laÅŸtÄ±rma
        preprocess: Ses Ã¶n iÅŸleme aktif mi
        engine: KullanÄ±lacak motor
        
    Returns:
        TranscriptionResult: DetaylÄ± sonuÃ§ objesi
    """
    start_time = time.time()
    
    # Ses dosyasÄ± bilgilerini al
    audio_info = _get_audio_info(path)
    processed_path = path
    
    # Ã–n iÅŸleme
    if preprocess and quality != "fastest":
        processed_path = _preprocess_audio(path)
        # Ä°ÅŸlenmiÅŸ dosya iÃ§in bilgileri gÃ¼ncelle
        audio_info = _get_audio_info(processed_path)
    
    # Model parametrelerini belirle (Ã¶nce manual deÄŸerler, sonra quality ayarlarÄ±)
    if model_name:
        model_size = model_name
    elif quality == "fastest":
        model_size = "tiny"
    elif quality == "balanced":
        model_size = "medium"
    elif quality == "highest":
        model_size = "large-v3"
    elif quality == "ultra":
        model_size = "large-v3"  # Ultra mode with enhanced parameters
    else:  # hybrid
        return _transcribe_hybrid_advanced(processed_path, language, audio_info, **kwargs)
    
    # Beam size belirleme
    if beam_size is None:
        if quality == "fastest":
            beam_size = 1
        elif quality == "balanced":
            beam_size = 2
        elif quality == "highest":
            beam_size = 5
        elif quality == "ultra":
            beam_size = 10  # Ultra high beam size for maximum accuracy
        else:
            beam_size = 1
    
    # VAD ayarlarÄ±
    if vad_threshold is None:
        vad_threshold = 0.5  # VarsayÄ±lan
        if long_form:
            vad_threshold = 0.3  # Uzun kayÄ±tlarda daha hassas
    
    enable_vad = quality != "fastest"
    
    # Adaptif parametreler
    adaptive_params = _adaptive_parameters(audio_info)
    
    # Engine seÃ§imi
    if engine == "auto":
        # Ses Ã¶zelliklerine gÃ¶re otomatik seÃ§im
        if audio_info.snr_estimate and audio_info.snr_estimate < 15:
            engine = "whisper"  # DÃ¼ÅŸÃ¼k kalitede Whisper daha iyi
        elif audio_info.duration > 3600:  # 1 saat+
            engine = "azure"  # Uzun kayÄ±tlarda cloud servisler daha stabil
        else:
            engine = "whisper"
    
    # Ultra mode optimizasyonlarÄ±
    if quality == "ultra":
        # Maximum accuracy iÃ§in Ã¶zel parametreler
        transcribe_kwargs = {
            'temperature': [0.0, 0.2, 0.4, 0.6, 0.8],  # Multiple temperature sampling
            'best_of': 5,  # En iyi 5 denemeden seÃ§
            'beam_size': beam_size,
            'patience': 2.0,  # Daha sabÄ±rlÄ± decode
            'suppress_tokens': [-1],  # Ã–zel token bastÄ±rma
            'condition_on_previous_text': True,
            'compression_ratio_threshold': 2.4,
            'logprob_threshold': -1.0,
            'no_speech_threshold': 0.6,
            'word_timestamps': True,  # Ultra modda kelime zamanlarÄ±
            'prepend_punctuations': "\"'([{-",
            'append_punctuations': "\"'.,:!?)]}"
        }
    else:
        # Uzun kayÄ±t optimizasyonlarÄ±
        transcribe_kwargs = kwargs.copy()
        if long_form:
            # Uzun kayÄ±tlar iÃ§in Ã¶zel parametreler
            transcribe_kwargs.update({
                'chunk_length_s': 30,  # 30 saniyelik parÃ§alar
                'batch_size': 4,  # Daha bÃ¼yÃ¼k batch
                'progress': True,  # Ä°lerleme gÃ¶ster
            })
            if enable_vad:
                transcribe_kwargs.update({
                    'vad_min_silence_ms': 500,  # Daha uzun sessizlik gerekli
                    'vad_window_ms': 30  # Daha bÃ¼yÃ¼k pencere
                })

    # Transkripsiyon yap
    if engine == "whisper":
        raw_result = transcribe(
            processed_path,
            language=language,
            model_size=model_size,
            device=device,
            beam_size=adaptive_params.get('beam_size', beam_size),
            **transcribe_kwargs
        )
        model_used = f"whisper-{model_size}"
    elif engine == "azure":
        raw_result = _transcribe_azure(processed_path, language)
        model_used = "azure-speech"
    elif engine == "google":
        raw_result = _transcribe_google(processed_path, language)
        model_used = "google-speech"
    else:
        raise ValueError(f"Bilinmeyen engine: {engine}")
    
    # GÃ¼ven skoru hesapla
    confidence = _calculate_confidence(raw_result.get("segments", []), model_used)
    
    # Kalite metrikleri
    quality_metrics = {
        "word_count": len(raw_result.get("text", "").split()),
        "segment_count": len(raw_result.get("segments", [])),
        "avg_segment_length": np.mean([
            len(s.get("text", "").split()) 
            for s in raw_result.get("segments", [])
        ]) if raw_result.get("segments") else 0,
        "processing_speed_rtf": (time.time() - start_time) / audio_info.duration if audio_info.duration > 0 else 0
    }
    
    # Ä°Ã§erik tipine gÃ¶re ek iÅŸlemler
    segments = raw_result.get("segments", [])
    text = raw_result.get("text", "")
    
    if content_type == "meeting":
        # ToplantÄ± iÃ§in speaker labels ekle
        segments = _enhance_meeting_segments(segments)
    elif content_type == "lecture":
        # Ders iÃ§in konu baÅŸlÄ±klarÄ± belirle
        segments = _enhance_lecture_segments(segments)
    
    # Sonucu yapÄ±landÄ±r - dict dÃ¶ndÃ¼r (TranscriptionResult yerine uyumluluk iÃ§in)
    result_dict = {
        'text': text,
        'segments': segments,
        'duration': raw_result.get("duration", audio_info.duration),
        'confidence': confidence,
        'audio_quality': _assess_audio_quality(audio_info),
        'model_used': model_used,
        'processing_time': time.time() - start_time,
        'quality_metrics': quality_metrics
    }
    
    # GeÃ§ici dosyayÄ± temizle
    if processed_path != path and os.path.exists(processed_path):
        try:
            os.unlink(processed_path)
        except:
            pass
    
    return result_dict

def _transcribe_hybrid_advanced(
    path: str,
    language: str,
    audio_info: AudioInfo,
    **kwargs
) -> TranscriptionResult:
    """Hibrit model ile geliÅŸmiÅŸ transkripsiyon"""
    start_time = time.time()
    
    # FarklÄ± modelleri paralel Ã§alÄ±ÅŸtÄ±r
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = []
        
        # Whisper
        futures.append(
            executor.submit(transcribe, path, language=language, model_size="medium")
        )
        
        # Azure (varsa)
        if _HAS_AZURE and os.getenv("AZURE_SPEECH_KEY"):
            futures.append(
                executor.submit(_transcribe_azure, path, language)
            )
        
        # Google (varsa)  
        if _HAS_GOOGLE and os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
            futures.append(
                executor.submit(_transcribe_google, path, language)
            )
        
        # SonuÃ§larÄ± topla
        results = []
        for future in as_completed(futures, timeout=600):  # 10 dakika timeout
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                print(f"[STT][Hibrit] Model hatasÄ±: {e}")
    
    if not results:
        raise RuntimeError("Hibrit modelde hiÃ§bir model baÅŸarÄ±lÄ± olamadÄ±")
    
    # SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±r ve en iyisini seÃ§
    best_result = max(results, key=lambda r: len(r.get("text", "")))
    
    confidence = _calculate_confidence(best_result.get("segments", []), "hybrid")
    
    return TranscriptionResult(
        text=best_result.get("text", ""),
        segments=best_result.get("segments", []),
        duration=best_result.get("duration", audio_info.duration),
        confidence=confidence,
        audio_info=audio_info,
        model_used="hybrid-multi",
        processing_time=time.time() - start_time,
        quality_metrics={
            "models_tried": len(results),
            "best_model_score": len(best_result.get("text", ""))
        }
    )

# =============================================================================
#                      Ä°LERÄ° DÃœZEY POST-PROCESSING
# =============================================================================
def _advanced_text_correction(text: str, language: str = "tr") -> str:
    """GeliÅŸmiÅŸ metin dÃ¼zeltme ve normalizasyon"""
    if not text:
        return ""
    
    # 1. NLP modÃ¼lÃ¼ ile temel dÃ¼zeltme
    corrected = text
    if nlp and hasattr(nlp, "normalize_transcript"):
        try:
            corrected = nlp.normalize_transcript(text)
        except Exception:
            pass
    
    # 2. Dil Ã¶zel dÃ¼zeltmeler
    if language.startswith("tr"):
        # TÃ¼rkÃ§e Ã¶zel dÃ¼zeltmeler
        corrections = {
            # YaygÄ±n STT hatalarÄ±
            "bi": "bir", "biÅŸey": "bir ÅŸey", "hiÃ§bi": "hiÃ§bir",
            "nasÄ±": "nasÄ±l", "bÃ¶le": "bÃ¶yle", "ÅŸÃ¶le": "ÅŸÃ¶yle",
            "deil": "deÄŸil", "oluo": "oluyor", "diyo": "diyor",
            "gelio": "geliyor", "gidio": "gidiyor",
            
            # Teknik terimler
            "veri tabanÄ±": "veritabanÄ±", "veri base": "veritabanÄ±",
            "data base": "veritabanÄ±", "deyÄ±tÄ±": "data",
            "paiton": "Python", "payton": "Python",
            "cubernets": "Kubernetes", "kubernetis": "Kubernetes",
            
            # YaygÄ±n kelimeler
            "toplantÄ±mÄ±z": "toplantÄ±mÄ±z", "gÃ¶rÃ¼ÅŸmemiz": "gÃ¶rÃ¼ÅŸmemiz",
        }
        
        for wrong, right in corrections.items():
            corrected = re.sub(rf"\b{re.escape(wrong)}\b", right, corrected, flags=re.IGNORECASE)
    
    # 3. TekrarlarÄ± temizle
    corrected = _remove_advanced_duplicates(corrected)
    
    # 4. CÃ¼mle yapÄ±sÄ±nÄ± dÃ¼zelt
    corrected = _fix_sentence_structure(corrected)
    
    return corrected.strip()

def _remove_advanced_duplicates(text: str) -> str:
    """GeliÅŸmiÅŸ tekrar temizleme"""
    sentences = re.split(r'[.!?]+', text)
    cleaned = []
    
    for i, sent in enumerate(sentences):
        sent = sent.strip()
        if not sent:
            continue
            
        # Ã–nceki cÃ¼mle ile benzerlik kontrolÃ¼
        is_duplicate = False
        if i > 0 and cleaned:
            prev_sent = cleaned[-1]
            similarity = _text_similarity(sent, prev_sent)
            if similarity > 0.8:  # %80+ benzer
                is_duplicate = True
        
        if not is_duplicate:
            cleaned.append(sent)
    
    return '. '.join(cleaned) + ('.' if cleaned else '')

def _text_similarity(text1: str, text2: str) -> float:
    """Ä°ki metin arasÄ±ndaki benzerlik oranÄ±"""
    if not text1 or not text2:
        return 0.0
    
    if _HAS_FUZZ:
        return fuzz.ratio(text1.lower(), text2.lower()) / 100.0
    
    # Basit jaccard benzerliÄŸi
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    intersection = len(words1.intersection(words2))
    union = len(words1.union(words2))
    return intersection / union if union > 0 else 0.0

def _fix_sentence_structure(text: str) -> str:
    """CÃ¼mle yapÄ±sÄ±nÄ± dÃ¼zelt"""
    # Noktalama dÃ¼zeltmeleri
    text = re.sub(r'\s+([,.!?;:])', r'\1', text)  # Noktalama Ã¶ncesi boÅŸluk
    text = re.sub(r'([,.!?;:])\s*', r'\1 ', text)  # Noktalama sonrasÄ± boÅŸluk
    text = re.sub(r'\s+', ' ', text)  # Ã‡oklu boÅŸluklarÄ± tek yap
    
    # CÃ¼mle baÅŸlarÄ±nÄ± bÃ¼yÃ¼k yap
    sentences = re.split(r'([.!?]+)', text)
    fixed = []
    
    for i, part in enumerate(sentences):
        if i % 2 == 0 and part.strip():  # CÃ¼mle metni
            part = part.strip()
            if part:
                part = part[0].upper() + part[1:] if len(part) > 1 else part.upper()
                fixed.append(part)
        elif part.strip():  # Noktalama
            fixed.append(part)
    
    return ''.join(fixed)

def _extract_speaker_segments(segments: List[Dict]) -> List[Dict]:
    """KonuÅŸmacÄ± segmentasyonu (basit versiyon)"""
    # Bu gerÃ§ek bir speaker diarization sistemi deÄŸil
    # Sadece sessizlik bazlÄ± segment ayÄ±rma
    
    enhanced_segments = []
    current_speaker = "KonuÅŸmacÄ±_1"
    speaker_counter = 1
    
    for i, seg in enumerate(segments):
        if i > 0:
            prev_end = segments[i-1].get("end", 0)
            current_start = seg.get("start", 0)
            pause_duration = current_start - prev_end
            
            # 3 saniyeden uzun ara varsa konuÅŸmacÄ± deÄŸiÅŸti kabul et
            if pause_duration > 3.0:
                speaker_counter += 1
                current_speaker = f"KonuÅŸmacÄ±_{speaker_counter}"
        
        enhanced_seg = dict(seg)
        enhanced_seg["speaker"] = current_speaker
        enhanced_segments.append(enhanced_seg)
    
    return enhanced_segments

def _quality_assessment(result: TranscriptionResult) -> Dict[str, Any]:
    """Transkripsiyon kalitesini deÄŸerlendir"""
    assessment = {
        "overall_score": 0.0,
        "text_quality": 0.0,
        "timing_accuracy": 0.0,
        "completeness": 0.0,
        "issues": []
    }
    
    text = result.text
    segments = result.segments
    
    # 1. Metin kalitesi
    if text:
        word_count = len(text.split())
        # Ã‡ok kÄ±sa metinler ÅŸÃ¼pheli
        if word_count < 10:
            assessment["issues"].append("Ã‡ok kÄ±sa transkripsiyon")
            assessment["text_quality"] = 0.3
        else:
            # Ã–zel terim varlÄ±ÄŸÄ±
            terms_found = sum(1 for term in _load_custom_terms() 
                            if term.lower() in text.lower())
            term_ratio = terms_found / max(1, word_count / 50)  # 50 kelimede 1 terim bekleniyor
            
            # Tekrar oranÄ±
            unique_words = len(set(text.lower().split()))
            repeat_ratio = 1 - (unique_words / word_count) if word_count > 0 else 0
            
            assessment["text_quality"] = min(1.0, 0.7 + 0.2 * min(1, term_ratio) - 0.3 * repeat_ratio)
    
    # 2. Zamanlama doÄŸruluÄŸu
    if segments:
        timing_issues = 0
        for i, seg in enumerate(segments):
            start = seg.get("start", 0)
            end = seg.get("end", 0)
            
            if end <= start:
                timing_issues += 1
                
            if i > 0:
                prev_end = segments[i-1].get("end", 0)
                if start < prev_end:  # Overlap
                    timing_issues += 1
        
        assessment["timing_accuracy"] = max(0, 1 - (timing_issues / len(segments)))
        
        if timing_issues > len(segments) * 0.1:  # %10'dan fazla problem
            assessment["issues"].append("Zamanlama sorunlarÄ±")
    
    # 3. TamlÄ±k
    expected_duration = result.duration
    if expected_duration > 0 and segments:
        covered_time = segments[-1].get("end", 0) - segments[0].get("start", 0)
        coverage = covered_time / expected_duration
        assessment["completeness"] = min(1.0, coverage)
        
        if coverage < 0.8:
            assessment["issues"].append("Eksik ses analizi")
    
    # Genel skor
    assessment["overall_score"] = (
        0.5 * assessment["text_quality"] +
        0.3 * assessment["timing_accuracy"] + 
        0.2 * assessment["completeness"]
    )
    
    return assessment

def post_process_result(result: TranscriptionResult, language: str = "tr") -> TranscriptionResult:
    """SonuÃ§larÄ± geliÅŸmiÅŸ post-processing ile iyileÅŸtir"""
    
    # 1. Metin dÃ¼zeltme
    corrected_text = _advanced_text_correction(result.text, language)
    
    # 2. Segment metinlerini dÃ¼zelt
    corrected_segments = []
    for seg in result.segments:
        corrected_seg = dict(seg)
        corrected_seg["text"] = _advanced_text_correction(seg.get("text", ""), language)
        corrected_segments.append(corrected_seg)
    
    # 3. KonuÅŸmacÄ± bilgilerini ekle
    speaker_segments = _extract_speaker_segments(corrected_segments)
    
    # 4. Kalite deÄŸerlendirmesi
    enhanced_result = TranscriptionResult(
        text=corrected_text,
        segments=speaker_segments,
        duration=result.duration,
        confidence=result.confidence,
        audio_info=result.audio_info,
        model_used=result.model_used,
        processing_time=result.processing_time,
        word_level=result.word_level,
        speaker_info=[{"speaker": f"KonuÅŸmacÄ±_{i}", "segments": len([s for s in speaker_segments if s.get('speaker') == f"KonuÅŸmacÄ±_{i}"])} 
                     for i in range(1, max([int(s.get('speaker', 'KonuÅŸmacÄ±_1').split('_')[1]) for s in speaker_segments], default=[1]) + 1)],
        quality_metrics=result.quality_metrics
    )
    
    # Kalite deÄŸerlendirmesi ekle
    quality_assessment = _quality_assessment(enhanced_result)
    if enhanced_result.quality_metrics:
        enhanced_result.quality_metrics.update(quality_assessment)
    else:
        enhanced_result.quality_metrics = quality_assessment
    
    return enhanced_result

# =============================================================================
#                          KOLAY KULLANIM API'LERÄ°
# =============================================================================
def transcribe_simple(audio_path: str, language: str = "tr") -> str:
    """En basit kullanÄ±m: sadece metin dÃ¶ndÃ¼r"""
    result = transcribe_advanced(audio_path, language=language, quality="balanced")
    return result.get('text', '') if isinstance(result, dict) else str(result)

def transcribe_with_speakers(audio_path: str, language: str = "tr") -> Dict[str, Any]:
    """KonuÅŸmacÄ± bilgileri ile transkripsiyon"""
    result = transcribe_advanced(audio_path, language=language, quality="highest")
    result = post_process_result(result, language)
    
    return {
        "text": result.text,
        "speakers": result.speaker_info,
        "segments": result.segments,
        "confidence": result.confidence
    }

def transcribe_for_meeting(audio_path: str, language: str = "tr") -> Dict[str, Any]:
    """ToplantÄ± kayÄ±tlarÄ± iÃ§in optimize edilmiÅŸ transkripsiyon"""
    result = transcribe_advanced(
        audio_path, 
        language=language, 
        quality="highest",
        preprocess=True,
        engine="auto"
    )
    result = post_process_result(result, language)
    
    # NLP modÃ¼lÃ¼ ile gÃ¶rev ve kararlarÄ± Ã§Ä±kar
    tasks, decisions = [], []
    if nlp:
        try:
            if hasattr(nlp, "extract_tasks"):
                tasks = nlp.extract_tasks(result.text)
            if hasattr(nlp, "extract_decisions"):
                decisions = nlp.extract_decisions(result.text)
        except Exception:
            pass
    
    return {
        "transcript": result.text,
        "segments": result.segments,
        "speakers": result.speaker_info,
        "tasks": tasks,
        "decisions": decisions,
        "confidence": result.confidence,
        "quality_score": result.quality_metrics.get("overall_score", 0.0) if result.quality_metrics else 0.0,
        "processing_time": result.processing_time
    }

# Geriye uyumluluk iÃ§in eski API
def transcribe_file(path: str, **kwargs) -> Dict:
    """Eski API ile uyumluluk"""
    result = transcribe_advanced(path, **kwargs)
    return asdict(result)
